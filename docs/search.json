[
  {
    "objectID": "notebooks/normal_mixture.html",
    "href": "notebooks/normal_mixture.html",
    "title": "Self tuning penalty",
    "section": "",
    "text": "We implement the scale mixture of normals prior. In particular given a function for computing the marginal likelihood \\(\\ell(z, s)\\) we can use autograd to (1) get functions for the posterior mean and variance, and (2) get gradients for the posterior mean and variance with respect to \\(z, s\\).\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax.scipy.stats import norm\nfrom jax.scipy.special import logsumexp\nimport jax\nimport matplotlib.pyplot as plt\nimport jaxopt\n\njax.config.update(\"jax_enable_x64\", True)\n# here we check that the gradient based computations work correctly for a simple test case\n# y \\sim N(b, 1), b \\sim (0, 1)\nnormalize = lambda x: np.exp(x - logsumexp(x))\n\ndef log_marginal_normal(z, s, sigma=1.0):\n    return norm.logpdf(z, loc=0., scale=jnp.sqrt(s**2 + sigma**2))\n\n# compute posterior mean and variance by tweedie's formula\nposterior_mean = lambda z, s: z + s**2 * jax.grad(log_marginal_normal)(z, s, 1.)\nposterior_variance = lambda z, s: s**2 * (1 + s**2 * jax.grad(jax.grad(log_marginal_normal))(z, s, 1.))\n\nz = 3.\ns = 1.\n# 3/2, 1 / (1 + 1)\nposterior_mean(z, s), posterior_variance(z, s)\n\nz = 1.\ns = np.sqrt(0.5)\n# 2/3, 1 / (1 + 2)\nposterior_mean(z, s), posterior_variance(z, s)\n\n(Array(0.66666667, dtype=float64), Array(0.33333333, dtype=float64))\nfrom functools import partial\n\n# g is a scale mixture of normals (possibly with a point mass at 0)\ndef log_marginal(z, s, pi, sigma_grid):\n    return logsumexp(jnp.log(pi) + norm.logpdf(z, loc=0., scale=jnp.sqrt(s**2 + sigma_grid**2)))\n\npi = np.ones(5)/5\nsigma_grid = np.arange(5) + 1\nlog_marginal_g = partial(log_marginal, pi=pi, sigma_grid=sigma_grid)\n\n# this is for the gaussian error model\nposterior_mean = lambda z, s: z + s**2 * jax.grad(log_marginal_g)(z, s)\nposterior_variance = lambda z, s: s**2 * (1. + s**2 * jax.grad(jax.grad(log_marginal_g))(z, s))\n\n\nkappa = lambda z, s: log_marginal_g(z, s) - norm.logpdf(z, 0, s)\nEeta = lambda z, s: jax.grad(kappa)(z, s)\nEeta2 = lambda z, s: Eeta(z, s)**2 + jax.grad(Eeta)(z,s)\nKL = lambda z, s: z * Eeta(z, s) - Eeta2(z, s) * s**2/2 - kappa(z, s)\n\n#KL = lambda: z, s: z * posterior_mean(z, s)/s**2 - 0.5 * posterior_variance(z, s)/s**2\n# this would work substituting other exponential families\nposterior_mean2 = lambda z, s: s**2 * (jax.grad(log_marginal_g)(z, s) - jax.grad(norm.logpdf)(z, 0, s))\nposterior_variance2 = lambda z, s: s**4 * (jax.grad(jax.grad(log_marginal_g))(z, s) - jax.grad(jax.grad(norm.logpdf))(z, 0, s))\n# test relationship between tweedies formula and general exponential family version\ns = 4.\nposterior_mean(1., s) - s**2 * Eeta(1., s)\n\nArray(0., dtype=float64, weak_type=True)\n# test relationship between tweedies formula and general exponential family version\nposterior_variance2 = lambda z, s: s**4 * Eeta2(z, s) - (s**2 * Eeta(z, s))**2\nz, s = 1., 4.\nposterior_variance(z, s) - posterior_variance2(z, s)\n\nArray(0., dtype=float64, weak_type=True)\n# this is for the gaussian error model\ndef make_functions(log_marginal_g):\n    # tweedie type formula for posterior mean and variance\n    posterior_mean = lambda z, s: z + s**2 * jax.grad(log_marginal_g)(z, s)\n    posterior_variance = lambda z, s: s**2 * (1. + s**2 * jax.grad(jax.grad(log_marginal_g))(z, s))\n\n    # natural parameter\n    kappa = lambda z, s: log_marginal_g(z, s) - norm.logpdf(z, 0, s)\n    Eeta = lambda z, s: jax.grad(kappa)(z, s)\n    Eeta2 = lambda z, s: Eeta(z, s)**2 + jax.grad(Eeta)(z,s)\n    KL = lambda z, s: z * Eeta(z, s) - 0.5 * Eeta2(z, s) * s**2 - kappa(z, s)\n    \n    def Eqlogp(z, s, X, y, tau):\n        mu = jax.vmap(posterior_mean)(z, s)\n        V = jax.vmap(posterior_variance)(z, s)\n        yhat = X @ mu\n        d = jnp.diag(X.T @ X)\n        return - 0.5 * tau * (jnp.sum((y - yhat)**2) + jnp.sum(d * V))\n    \n    def elbo(z, s, X, y, tau):\n        return Eqlogp(z, s, X, y, tau) - jnp.sum(jax.vmap(KL)(z, s))\n\n    def posterior(nu):\n        z = nu['z']\n        s = jnp.log(1 + jnp.exp(nu['log1exps']))\n        mu = jax.vmap(posterior_mean)(z, s)\n        var = jax.vmap(posterior_variance)(z, s)\n        return mu, var\n\n    def objective(nu, X, y, tau):\n        z = nu['z']\n        s = jnp.log(1 + jnp.exp(nu['log1exps']))\n        return -elbo(z, s, X, y, tau)\n    \n    def fit_lbfgs(y, X, tau, maxiter=100):\n        p = X.shape[1]\n        solver = jaxopt.ScipyMinimize(fun=objective, maxiter=maxiter)\n        nu = dict(z=np.zeros(p), log1exps=np.ones(p)) # initialize variational parameters\n        res = solver.run(nu, X=X, y=y, tau=tau) # optimize  \n        mu, var = posterior(res.params)\n        return dict(res=res, mu=mu, var=var)\n        \n    return fit_lbfgs, objective, posterior, KL"
  },
  {
    "objectID": "notebooks/normal_mixture.html#gaussian-error-gaussian-prior",
    "href": "notebooks/normal_mixture.html#gaussian-error-gaussian-prior",
    "title": "Self tuning penalty",
    "section": "Gaussian error, gaussian prior",
    "text": "Gaussian error, gaussian prior\nIn this special case, we can implement the CAVI updates very easily in closed form. We see good agreement between the CAVI results and gradient descent on the compound objective.\n\nFixed \\(g\\)\n\n# mean field updates\n# simulate\ndef simulate_gaussian(n, p, tau=1, tau0=1., seed=1):\n    np.random.seed(seed)\n    X = np.random.normal(size=(n, p))\n    b = np.random.normal(size=p) / np.sqrt(tau0)\n    y = X @ b + np.random.normal(size=n)/np.sqrt(tau)\n    return dict(y=y, X=X, b=b, tau=tau, tau0=tau0, seed=seed)\n\ndef kl_univariate_gaussian(muq, varq, varg):\n    return 0.5 * np.sum((varq + muq**2)/varg + np.log(varg/varq)  - 1)\n\ndef elbo_cavi(mu, var, y, X, tau, tau0):\n    ybar = X @ mu\n    d = np.diag(X.T @ X)\n    #kl = 0.5 * np.sum((var + mu)**2 * tau0 + np.log(tau0 * var)  - 1)\n    kl = kl_univariate_gaussian(mu, var, 1/tau0)\n    Eloglik =  -0.5 * tau * np.sum((y-ybar)**2) - 0.5 * tau * np.sum(d * var)\n    return Eloglik - kl\n\ndef gaussian_mr_cavi(y, X, tau, tau0, maxiter=100):\n    n, p = X.shape\n    mu = np.zeros(X.shape[1])\n    var = np.ones(X.shape[1])\n    d = np.diag(X.T @ X)\n    r = y - X @ mu\n    elbos = [elbo_cavi(mu, var, y, X, tau, tau0)]\n    for i in range(maxiter):\n        for j in range(p):\n            r = r + X[:, j] * mu[j]\n            shrink = tau * d[j] /(tau * d[j] + tau0) \n            bhat = np.sum(r * X[:, j]) / d[j]\n            mu[j] = shrink * bhat\n            var[j] = 1/(tau * d[j] + tau0)\n            r = r - X[:,j] * mu[j]\n        elbos.append(elbo_cavi(mu, var, y, X, tau, tau0))\n    return dict(mu=mu, var=var, elbo=np.array(elbos), tau=tau, tau0=tau0)\n\n\n# the larger the prior precision, the more aggressive shrinkage.\nsim = simulate_gaussian(1000, 10, tau=1., tau0=1.)\nget_range = lambda x: [x.min(), x.max()]\n\ny, X, b = sim['y'], sim['X'], sim['b']\ncavi_fit1 = gaussian_mr_cavi(y, X, 1., 1000.)\ncavi_fit2 = gaussian_mr_cavi(y, X, 1., 100.)\ncavi_fit3 = gaussian_mr_cavi(y, X, 1., 10.)\nplt.scatter(b, cavi_fit1['mu'])\nplt.scatter(b, cavi_fit2['mu'])\nplt.scatter(b, cavi_fit3['mu'])\nplt.plot(get_range(b), get_range(b), c='k', linestyle='dotted')\n\n\n\n\n\n\n\n\n\ndef log_marginal_normal(z, s, tau0):\n    return norm.logpdf(z, loc=0., scale=jnp.sqrt(s**2 + 1/tau0))\n\nmaxiter = 1000\nfit_lbfgs, objective, posterior, KL = make_functions(partial(log_marginal_normal, tau0=1000.))\ncompound_fit1 = fit_lbfgs(y, X, 1., maxiter)\n\nfit_lbfgs, objective, posterior, KL = make_functions(partial(log_marginal_normal, tau0=100.))\ncompound_fit2 = fit_lbfgs(y, X, 1., maxiter)\n\nfit_lbfgs, objective, posterior, KL = make_functions(partial(log_marginal_normal, tau0=10.))\ncompound_fit3 = fit_lbfgs(y, X, 1., maxiter)\n\nplt.scatter(b, compound_fit1['mu'])\nplt.scatter(b, compound_fit2['mu'])\nplt.scatter(b, compound_fit3['mu'])\nplt.plot(get_range(b), get_range(b), c='k', linestyle='dotted')\nplt.ylim(get_range(b))\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(2, 3)\n\nplt.sca(ax[0,0])\nplt.scatter(cavi_fit1['mu'], compound_fit1['mu'])\nplt.plot(get_range(cavi_fit1['mu']), get_range(cavi_fit1['mu']), color='k', linestyle='dotted')\nplt.title('mean, tau0 = 1000')\nplt.xlabel('CAVI')\nplt.ylabel('compound')\n\nplt.sca(ax[0,1])\nplt.scatter(cavi_fit2['mu'], compound_fit2['mu'])\nplt.plot(get_range(cavi_fit2['mu']), get_range(cavi_fit2['mu']), color='k', linestyle='dotted')\nplt.title('mean, tau0 = 100')\nplt.xlabel('CAVI')\nplt.ylabel('compound')\n\nplt.sca(ax[0,2])\nplt.scatter(cavi_fit3['mu'], compound_fit3['mu'])\nplt.plot(get_range(cavi_fit3['mu']), get_range(cavi_fit3['mu']), color='k', linestyle='dotted')\nplt.title('mean, tau0 = 10')\nplt.xlabel('CAVI')\nplt.ylabel('compound')\n\n\nplt.sca(ax[1,0])\nplt.scatter(cavi_fit1['var'], compound_fit1['var'])\nplt.plot(get_range(cavi_fit1['var']), get_range(cavi_fit1['var']), color='k', linestyle='dotted')\nplt.title('var, tau0 = 1000')\nplt.xlabel('CAVI')\nplt.ylabel('compound')\n\nplt.sca(ax[1,1])\nplt.scatter(cavi_fit2['var'], compound_fit2['var'])\nplt.plot(get_range(cavi_fit2['var']), get_range(cavi_fit2['var']), color='k', linestyle='dotted')\nplt.title('mean, tau0 = 100')\nplt.xlabel('CAVI')\nplt.ylabel('compound')\n\nplt.sca(ax[1,2])\nplt.scatter(cavi_fit3['var'], compound_fit3['var'])\nplt.plot(get_range(cavi_fit3['var']), get_range(cavi_fit3['var']), color='k', linestyle='dotted')\nplt.title('mean, tau0 = 10')\nplt.xlabel('CAVI')\nplt.ylabel('compound')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nprint(f\"CAVI solution ELBO = {cavi_fit1['elbo'][-1]:.2f}, Combound solution ELBO = {-compound_fit1['res'].state.fun_val:.2f}\")\n\nCAVI solution ELBO = -982.67, Combound solution ELBO = -982.67\n\n\n\n\nEstimate prior variance\nNow we will try to estimate the prior variance. There is only one parameter controlling the prior family, namely, the prior variance. A simple strategy is to look across a fixed grid of values. Then we will try to optimize the ELBO w.r.t the prior variance through gradient based optimization.\n\n# this is for the gaussian error model\ndef make_functions2(log_marginal_g, transforms):\n    # tweedie type formula for posterior mean and variance\n    posterior_mean = lambda z, s, params: z + s**2 * jax.grad(log_marginal_g)(z, s, **params)\n    posterior_variance = lambda z, s, params: s**2 * (1. + s**2 * jax.grad(jax.grad(log_marginal_g))(z, s, **params))\n\n    # natural parameter\n    kappa = lambda z, s, params: log_marginal_g(z, s, **params) - norm.logpdf(z, 0, s)\n    Eeta = lambda z, s, params: jax.grad(kappa)(z, s, params)\n    Eeta2 = lambda z, s, params: Eeta(z, s, params)**2 + jax.grad(Eeta)(z,s, params)\n    KL = lambda z, s, params: z * Eeta(z, s, params) - 0.5 * Eeta2(z, s, params) * s**2 - kappa(z, s, params)\n    \n    def Eqlogp(z, s, params, X, y, tau):\n        mu = jax.vmap(posterior_mean, (0, 0, None))(z, s, params)\n        V = jax.vmap(posterior_variance, (0, 0, None))(z, s, params)\n        yhat = X @ mu\n        d = jnp.diag(X.T @ X)\n        return - 0.5 * tau * (jnp.sum((y - yhat)**2) + jnp.sum(d * V))\n    \n    def elbo(z, s, params, X, y, tau):\n        return Eqlogp(z, s, params, X, y, tau) - jnp.sum(jax.vmap(KL, (0, 0, None))(z, s, params))\n\n    def posterior(nu):\n        z = nu['z']\n        s = jnp.log(1 + jnp.exp(nu['s']))\n        params = {k: transforms[k](v) for k, v in nu['params'].items()}\n        mu = jax.vmap(posterior_mean, (0, 0, None))(z, s, params)\n        var = jax.vmap(posterior_variance, (0, 0, None))(z, s, params)\n        return mu, var\n\n    # we can take gradients with respect to nu, which includes variational parameters and the parameters of the prior\n    def objective(nu, X, y, tau):\n        z = nu['z']\n        s = jnp.log(1 + jnp.exp(nu['s']))\n        params = {k: transforms[k](v) for k, v in nu['params'].items()}\n        return -elbo(z, s, params, X, y, tau)\n    \n    def fit_lbfgs(y, X, tau, nu_init, maxiter=100):\n        p = X.shape[1]\n        solver = jaxopt.ScipyMinimize(fun=objective, maxiter=maxiter)\n        nu = dict(z=np.zeros(p), log1exps=np.ones(p)) # initialize variational parameters\n        res = solver.run(nu_init, X=X, y=y, tau=tau) # optimize  \n        mu, var = posterior(res.params)\n        return dict(res=res, mu=mu, var=var)\n        \n    return fit_lbfgs, objective, posterior, KL\n\n\n# more variables here so that there are enough examples to learn the prior variance from.\nn, p = 1000, 100\nsim = simulate_gaussian(n, p, tau=1., tau0=10., seed=2)\nget_range = lambda x: [x.min(), x.max()]\ny, X, b = sim['y'], sim['X'], sim['b']\n\ntransforms = dict(\n    tau0 = lambda x: jnp.log(1 + jnp.exp(x))\n)\nnu_init = dict(\n    z = np.zeros(p),\n    s = np.ones(p),\n    params = dict(\n        tau0 = 10.\n    )\n)\ndef log_marginal_normal(z, s, tau0=1.0):\n    return norm.logpdf(z, loc=0., scale=jnp.sqrt(s**2 + 1/tau0))\n\n\ntau0s = 10**np.linspace(0, 1.5, 50)\ncavi_fits = [gaussian_mr_cavi(y, X, 1., tau0) for tau0 in tau0s]\nelbos = np.array([fit['elbo'][-1] for fit in cavi_fits])\n\n\nplt.plot(tau0s, elbos)\n\n\n\n\n\n\n\n\n\ndef fit_fixed_tau0(tau0):\n    maxiter = 10000\n    fit_lbfgs, objective, posterior, KL = make_functions(partial(log_marginal_normal, tau0=tau0))\n    return fit_lbfgs(y, X, 1., maxiter)\n\ncompound_fits = [fit_fixed_tau0(tau0) for tau0 in tau0s]\ncompound_elbos = np.array([fit['res'].state.fun_val for fit in compound_fits])\nplt.plot(tau0s, compound_elbos)\n\n\nsim = simulate_gaussian(1000, 100, tau=1., tau0=1., seed=1)\nget_range = lambda x: [x.min(), x.max()]\n\ntau0 = 10.\n\ny, X, b = sim['y'], sim['X'], sim['b']\ncavi_fit = gaussian_mr_cavi(y, X, 1., tau0)\n\nmaxiter = 10000\nfit_lbfgs, objective, posterior, KL = make_functions(partial(log_marginal_normal, tau0=tau0))\ncompound_fit = fit_lbfgs(y, X, 1., maxiter)\n\ndef plot_mu(cavi_fit, compound_fit):\n    fig, ax = plt.subplots(1, 2)\n    plt.sca(ax[0])\n    plt.scatter(cavi_fit['mu'], compound_fit['mu'])\n    plt.plot(get_range(cavi_fit['mu']), get_range(cavi_fit['mu']), color='k', linestyle='dotted')\n    plt.title('Posterior Mean')\n    plt.ylabel('Compound')\n    plt.xlabel('CAVI')\n    \n    plt.sca(ax[1])\n    plt.scatter(cavi_fit['var'], compound_fit['var'])\n    plt.plot(get_range(cavi_fit['var']), get_range(cavi_fit['var']), color='k', linestyle='dotted')\n    plt.title('Posterior Variance')\n    plt.xlabel('CAVI')\n\n    plt.tight_layout()\n    plt.show()\n\nplot_mu(cavi_fit, compound_fit)\nprint(f\"CAVI solution ELBO = {cavi_fit['elbo'][-1]:.2f}, Combound solution ELBO = {-compound_fit['res'].state.fun_val:.2f}\")\n\n\n\n\n\n\n\n\nCAVI solution ELBO = -1019.20, Combound solution ELBO = -2636.74\n\n\n\ntau0 = 10.\ncavi_fit = gaussian_mr_cavi(y, X, 1., tau0)\ncompound_fit = fit_fixed_tau0(tau0)\n\n\ncavi_fit['elbo'][-1], compound_fit['res'].state.fun_val\n\n(np.float64(-736.332976336279),\n Array(3432.76481919, dtype=float64, weak_type=True))\n\n\n\nplt.scatter(cavi_fit['mu'], compound_fit['mu'])\n\n\n\n\n\n\n\n\n\nelbos2 = [-fit['res'].state.fun_val for fit in fits]\nplt.plot(elbos, elbos2)\n\n\n\n\n\n\n\n\n\nelbos2\n\n[Array(-2233.08828511, dtype=float64, weak_type=True),\n Array(-2096.18983947, dtype=float64, weak_type=True),\n Array(-774.02871917, dtype=float64, weak_type=True),\n Array(-803.84306293, dtype=float64, weak_type=True),\n Array(-800.70384499, dtype=float64, weak_type=True),\n Array(-797.59266662, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(-2046.6561363, dtype=float64, weak_type=True),\n Array(-1636.65300844, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(-1559.64058631, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(-2494.04162933, dtype=float64, weak_type=True),\n Array(-768.55212925, dtype=float64, weak_type=True),\n Array(-2166.01574364, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(-1744.26440574, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(-1443.48115985, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(-4456.63731925, dtype=float64, weak_type=True),\n Array(-4285.31342852, dtype=float64, weak_type=True),\n Array(-3657.86000308, dtype=float64, weak_type=True),\n Array(-3189.92972406, dtype=float64, weak_type=True),\n Array(-3960.20701852, dtype=float64, weak_type=True),\n Array(-3183.15418399, dtype=float64, weak_type=True),\n Array(-3757.99806481, dtype=float64, weak_type=True),\n Array(-3344.9100764, dtype=float64, weak_type=True),\n Array(-3150.94363406, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(-2949.25193201, dtype=float64, weak_type=True),\n Array(-771.22079598, dtype=float64, weak_type=True),\n Array(-823.78279764, dtype=float64, weak_type=True),\n Array(-1264.03373314, dtype=float64, weak_type=True),\n Array(-763.51500337, dtype=float64, weak_type=True),\n Array(-741.98594706, dtype=float64, weak_type=True),\n Array(-765.7230467, dtype=float64, weak_type=True),\n Array(-746.87932362, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(nan, dtype=float64, weak_type=True),\n Array(-824.88598536, dtype=float64, weak_type=True),\n Array(-790.06144132, dtype=float64, weak_type=True),\n Array(-780.6909028, dtype=float64, weak_type=True)]\n\n\n\nmaxiter = 100\n#fit = fit_lbfgs(y, X, 1., maxiter)\n#fit['res'].state.fun_val\n\ntransforms = dict(\n    tau0 = lambda x: jnp.log(1 + jnp.exp(x))\n)\nnu_init = dict(\n    z = np.zeros(p),\n    s = np.ones(p),\n    params = dict(\n        tau0 = 10.\n    )\n)\n\nfit_lbfgs, objective, posterior, KL = make_functions2(log_marginal_normal, transforms)\nres = fit_lbfgs(y, X, 1., nu_init, maxiter=10)\n\n\nres['res'].params['params']['tau0']\n\nArray(9.96947586, dtype=float64)\n\n\n\nres['res'].state.fun_val\n\nArray(5607.1786817, dtype=float64, weak_type=True)"
  },
  {
    "objectID": "notebooks/normal_mixture.html#different-error-models",
    "href": "notebooks/normal_mixture.html#different-error-models",
    "title": "Self tuning penalty",
    "section": "Different error models",
    "text": "Different error models\n\n# this is for the gaussian error model\ndef log_likelihood_normal(b, X, y, tau=1.0):\n    yhat = X@b\n    return 0.5 * jnp.log(tau/(2 * jnp.pi)) - 0.5 * tau * jnp.sum((y - yhat)**2)\n\ndef make_functions3(log_likelihood, log_marginal_g, transforms):\n    # tweedie type formula for posterior mean and variance\n    posterior_mean = lambda z, s, params, fixed_params: z + s**2 * jax.grad(log_marginal_g)(z, s, **params, **fixed_params)\n    posterior_variance = lambda z, s, params, fixed_params: s**2 * (1. + s**2 * jax.grad(jax.grad(log_marginal_g))(z, s, **params, **fixed_params))\n\n    # natural parameter\n    kappa = lambda z, s, params, fixed_params: log_marginal_g(z, s, **params, **fixed_params) - norm.logpdf(z, 0, s)\n    Eeta = lambda z, s, params, fixed_params: jax.grad(kappa)(z, s, params, fixed_params)\n    Eeta2 = lambda z, s, params, fixed_params: Eeta(z, s, params, fixed_params)**2 + jax.grad(Eeta)(z,s, params, fixed_params)\n    KL = lambda z, s, params, fixed_params: z * Eeta(z, s, params, fixed_params) - 0.5 * Eeta2(z, s, params, fixed_params) * s**2 - kappa(z, s, params, fixed_params)\n    \n    def Eqlogp(z, s, lparams, lparams_fixed, gparams, gparams_fixed, X, y):\n        mu = jax.vmap(posterior_mean, (0, 0, None, None))(z, s, gparams, gparams_fixed)\n        V = jax.vmap(posterior_variance, (0, 0, None, None))(z, s, gparams, gparams_fixed)\n        # possible speed up: https://github.com/jax-ml/jax/issues/3801#issuecomment-2155354413\n        D = jnp.diag(jax.hessian(log_likelihood)(mu, X, y, **lparams, **lparams_fixed))\n        return log_likelihood(mu, X, y, **lparams, **lparams_fixed) + jnp.sum(D * V)\n    \n    def elbo(z, s, lparams, lparams_fixed, gparams, gparams_fixed, X, y):\n        return Eqlogp(z, s, lparams, lparams_fixed, gparams, gparams_fixed, X, y) - jnp.sum(jax.vmap(KL, (0, 0, None, None))(z, s, gparams, gparams_fixed))\n\n    def posterior(params, params_fixed):\n        z = params['z']\n        s = jnp.log(1 + jnp.exp(params['s']))\n        gparams = {k: transforms[k](v) for k, v in params['gparams'].items()}\n        gparams_fixed = params_fixed['gparams']\n        mu = jax.vmap(posterior_mean, (0, 0, None, None))(z, s, gparams, gparams_fixed)\n        var = jax.vmap(posterior_variance, (0, 0, None, None))(z, s, gparams, gparams_fixed)\n        return mu, var\n\n    # we can take gradients with respect to nu, which includes variational parameters and the parameters of the prior\n    def objective(params, params_fixed, X, y):\n        z = params['z']\n        s = jnp.log(1 + jnp.exp(params['s']))\n        lparams = {k: transforms[k](v) for k, v in params['lparams'].items()}\n        lparams_fixed = params_fixed['lparams']\n        gparams = {k: transforms[k](v) for k, v in params['gparams'].items()}\n        gparams_fixed = params_fixed['gparams']\n        return -elbo(z, s, lparams, lparams_fixed, gparams, gparams_fixed, X, y)\n    \n    def fit(y, X, params_init, params_fixed, maxiter=100):\n        p = X.shape[1]\n        solver = jaxopt.ScipyMinimize(fun=objective, maxiter=maxiter)\n        res = solver.run(params_init, X=X, y=y, params_fixed=params_fixed) # optimize  \n        mu, var = posterior(res.params, params_fixed)\n        return dict(res=res, mu=mu, var=var)\n        \n    return fit, objective, posterior, KL\n\n\ntransforms = dict(\n    tau0 = lambda x: jnp.log(1 + jnp.exp(x))\n)\n\nparams_init = dict(\n    z = np.zeros(p),\n    s = np.ones(p),\n    gparams = dict(),\n    lparams = dict(),\n)\n\nparams_fixed = dict(\n    lparams = dict(tau = 0.01), # residual error precision\n    gparams = dict(tau0 = 10.) # prior effect precision\n)\n\nfit_fun, objective, posterior, KL = make_functions3(log_likelihood_normal, log_marginal_normal, transforms)\nsim = simulate_gaussian(1000, 100, tau=1., tau0=1., seed=1)\ny, X, b = sim['y'], sim['X'], sim['b']\nfit = fit_fun(y, X, params_init, params_fixed, maxiter=100)\ncavi_fit = gaussian_mr_cavi(y, X, tau=.01, tau0=10.0)\n\n\nplt.scatter(cavi_fit['mu'], fit['mu'])\n\n\n\n\n\n\n\n\n\n# mean field updates\n# simulate\ndef log_likelihood_logistic(b, X, y):\n    eta = X@b\n    return jnp.sum(y * eta - jnp.log(1 + jnp.exp(eta)))\n    \ndef simulate_logistic(n, p, tau0=1., seed=1):\n    np.random.seed(seed)\n    X = np.random.normal(size=(n, p))\n    b = np.random.normal(size=p) / np.sqrt(tau0)\n    logit = X @ b\n    p = 1 / (1 + np.exp(-logit))\n    y = np.random.binomial(1, p)\n    return dict(y=y, X=X, b=b, tau0=tau0, seed=seed)\n    \n    transforms = dict(\n    tau0 = lambda x: jnp.log(1 + jnp.exp(x))\n)\n\nparams_init = dict(\n    z = np.zeros(p),\n    s = np.ones(p),\n    gparams = dict(),\n    lparams = dict(),\n)\n\nparams_fixed = dict(\n    lparams = dict(), # residual error precision\n    gparams = dict(tau0 = 10.) # prior effect precision\n)\n\nfit_fun, objective, posterior, KL = make_functions3(log_likelihood_logistic, log_marginal_normal, transforms)\n\nsim = simulate_logistic(1000, 100, tau0=10., seed=11)\ny, X, b = sim['y'], sim['X'], sim['b']\nfit_logistic = fit_fun(y, X, params_init, params_fixed, maxiter=100)\nplt.scatter(b, compound_fit_logistic['mu'])\nplt.plot(get_range(b), get_range(b), color='k', linestyle='dotted')"
  },
  {
    "objectID": "notebooks/normal_mixture.html#scale-mixture-of-normals",
    "href": "notebooks/normal_mixture.html#scale-mixture-of-normals",
    "title": "Self tuning penalty",
    "section": "Scale mixture of normals",
    "text": "Scale mixture of normals\n\n# g is a scale mixture of normals (possibly with a point mass at 0)\ndef log_marginal(z, s, pi, sigma_grid):\n    return logsumexp(jnp.log(pi) + norm.logpdf(z, loc=0., scale=jnp.sqrt(s**2 + sigma_grid**2)))\n\ndef simulate(n, p, tau=1, pi=np.ones(2)*0.5, sigma_grid=np.array([0., 1.]),seed=1):\n    np.random.seed(seed)\n    X = np.random.normal(size=(n, p))\n    b = np.random.normal(size=p) * np.random.choice(sigma_grid, p, replace=True, p=pi)\n    y = X @ b + np.random.normal(size=n)/np.sqrt(tau)\n    return dict(y=y, X=X, b=b, tau=tau, pi=pi, sigma_grid=sigma_grid, seed=seed)\n\n\n?np.random.sample\n\n\nSignature:   np.random.sample(*args, **kwargs)\nType:        cython_function_or_method\nString form: &lt;cyfunction sample at 0x11b03f510&gt;\nDocstring:  \nThis is an alias of `random_sample`. See `random_sample`  for the complete\ndocumentation.\n\n\n\n\nsim = simulate(1000, 1000)\n\n\nFixed \\(g\\)\n\nX, y, b = sim['X'], sim['y'], sim['b']\n\npi = np.ones(2) / 2\nsigma_grid = np.arange(2)\nlog_marginal_g = partial(log_marginal, pi=pi, sigma_grid=sigma_grid)\n\n\nmaxiter = 1000\nfit_lbfgs, objective, posterior, KL = make_functions(log_marginal_g)\ncompound_fit = fit_lbfgs(y, X, 1., maxiter)\n\nplt.scatter(b, compound_fit['mu'])\nplt.plot(get_range(b), get_range(b), c='k', linestyle='dotted')\nplt.ylim(get_range(b))\n\n\n\n\n\n\n\n\n\n\nEstimate \\(g\\): scale mixture of normals\nCan we estimate \\(\\pi\\)?\nFirst we rewrite the objective function generator so that it can take extra arguments.\n\nn, p = X.shape\npi = np.ones(2) / 2\nsigma_grid = np.arange(2)\n\n# still needs kwarg pi\nlog_marginal_g = partial(log_marginal, sigma_grid=sigma_grid)\n\n\nnormalize = lambda x: jnp.exp(x - logsumexp(x))\n\ntransforms = dict(\n    pi = lambda x: normalize(jnp.concat([jnp.zeros(1), x]))\n)\nnu_init = dict(\n    z = np.zeros(p),\n    s = np.ones(p),\n    params = dict(\n        pi = np.ones(1)\n    )\n)\n\nfit_lbfgs, objective, posterior, KL = make_functions2(log_marginal_g, transforms)\nres = fit_lbfgs(y, X, 1., nu_init, maxiter=10)\n\n\nres['res'].state.fun_val\n\nArray(257171.97473841, dtype=float64, weak_type=True)\n\n\n\nres['res'].params['params']['pi'], transforms['pi'](res['res'].params['params']['pi'])\n\n(Array([-57.26714742], dtype=float64),\n Array([1.00000000e+00, 1.34646131e-25], dtype=float64))\n\n\n\nmu, var = posterior(res['res'].params)\nplt.scatter(b, mu)\nplt.plot(get_range(b), get_range(b), color='k', linestyle='dotted')"
  },
  {
    "objectID": "notebooks/normal_mixture.html#logistic-regression",
    "href": "notebooks/normal_mixture.html#logistic-regression",
    "title": "Self tuning penalty",
    "section": "Logistic regression",
    "text": "Logistic regression\nThe other change we want to make is to\n\nz, s = compound_fit3['res'].params['z'], np.log(1 + np.exp(compound_fit3['res'].params['log1exps']))\njax.vmap(KL)(z, s).sum()\n\nArray(27.74933929, dtype=float64)\n\n\n\nmu, var = posterior(compound_fit3['res'].params)\nkl_univariate_gaussian(mu, var, 0.1)\n\nArray(27.74933929, dtype=float64)\n\n\n\nmu, var = compound_fit3['mu'], compound_fit3['var']\ncompound_fit3['res'].state.fun_val, -elbo_cavi(mu, var, y, X, 1., 10.), -elbo_cavi(cavi_fit3['mu'], cavi_fit3['var'], y, X, 1., 10.)\n\n(Array(523.85889506, dtype=float64, weak_type=True),\n Array(523.85889506, dtype=float64),\n np.float64(523.8588950591209))\n\n\n\nfit, ax = plt.subplots(1, 2)\nplt.sca(ax[0])\nplt.scatter(cavi_fit3['mu'], compound_fit3['mu'])\n\nplt.sca(ax[1])\nplt.scatter(cavi_fit3['var'], compound_fit3['var'])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nArray([-0.13567134,  0.26544161, -0.40584675, -0.85080322, -0.30657902,\n        0.12930233,  0.7454886 , -0.24539989, -0.26072837,  0.57516103],      dtype=float64)\n\n\n\nnp.arange(9).reshape((3, 3))[:, -1]\n\narray([2, 5, 8])\n\n\n\nobjective(nu, X, y), objective(res.params, X, y)\n\n(Array(11018.78375662, dtype=float64), Array(-1.32008246e+18, dtype=float64))\n\n\n\nz = res.params['z']\ns = np.exp(res.params['logs'])\njnp.sum(jax.vmap(KL)(z, s))\n\nArray(-5.08062333e+17, dtype=float64)\n\n\n\nEqlogp(z, s, X, y)\n\nArray(-10270147.73771521, dtype=float64)\n\n\n\n-elbo(z, s, X, y)\n\nArray(-5.08062333e+17, dtype=float64)\n\n\n\nobjective(res.params, X, y)\n\nArray(-1.32008246e+18, dtype=float64)\n\n\n\nres.params\n\n{'logs': Array([-32.65431832, -33.99137097, -32.41550529, -34.4223221 ,\n        -33.20838334, -33.72765494, -32.46054413, -33.43432489,\n        -35.88141027, -36.70303223], dtype=float64),\n 'z': Array([-16.64609876,  72.32127877, -65.75067104, -86.75400856,\n        -36.24869933,   2.84979608,  -5.43784215,  38.26865343,\n        -17.63772068,   3.67401776], dtype=float64)}\n\n\n\n\n\n\n\n\n\n\n\n\n\nobjective(res.params, X=X, y=y)\n\nArray(-1.17427129e+108, dtype=float64)\n\n\n\nposterior_variance2(1., 2.)\n\nArray(2.23706796, dtype=float64, weak_type=True)\n\n\n\n# the posterior variance computations doesjnp.sum(pi * sigma_grid**2), np.sum(post_assignment * post_var), posterior_variance(z, s)\n\n\n# posterior means agree\nz = 0.\ns = 10.\n\n# posterior distribution for normal means model when g = scale mixture of normals.\npost_mean = z * sigma_grid**2 / (s**2 + sigma_grid**2)\npost_var = s**2 * sigma_grid**2 / (s**2 + sigma_grid**2)\nlog_normalizing_const = norm.logpdf(z, 0, scale = np.sqrt(s**2 + sigma_grid**2))\npost_assignment = normalize(np.log(pi) + log_normalizing_const) # sums to one\neQ = - 0.5 * 1/s**2 * ((post_mean - z)**2 + post_var)\nkl_q_g = jnp.sum(post_assignment * eQ) - logsumexp(jnp.log(pi) + log_normalizing_const)\n\n# posterior mean computation is working\n# posterior variance computation does not seem to be working\nnp.sum(post_assignment * post_mean) - posterior_mean2(z, s), np.sum(post_assignment * post_var) - posterior_variance2(z, s)\n\n(Array(0., dtype=float64), Array(7.10542736e-15, dtype=float64))\n\n\n\n#posterior_mean2 = jax.vmap(lambda nu: posterior_mean(nu[0], jnp.exp(nu[1])))\n#posterior_variance2 = jax.vmap(lambda nu: posterior_variance(nu[0], jnp.exp(nu[1])))\nposterior_assignment = lambda: normalize(np.log(pi) + norm.logpdf(nu[0], 0, scale = np.sqrt(np.exp(nu[1]) + sigma_grid**2)))\n\ndef loglik(nu, y, X):\n    b = nu[:, 0]\n    v = jnp.exp(nu[:, 1])\n    d = jnp.diag(X.T @ X)\n    return -0.5 * jnp.sum(y - X@b) - 0.5 * jnp.sum(d * v)\n\ndef kl(nu):\n    post_mean = posterior_mean(nu)\n    post_var = posterior_variance(nu)\n    post_assingment = posterior_assignment(nu)\n    \n\n\nnu = np.array([z, np.log(s)])\nposterior_mean2(nu), posterior_variance2(nu), \n\n(Array(3.5166755, dtype=float32), Array(1.0133736, dtype=float32))\n\n\n\njax.grad(posterior_mean2)(nu)\n\nArray([ 0.2099328, -2.70177  ], dtype=float32)\n\n\n\njax.grad(posterior_variance2)(nu)\n\nArray([0.71372473, 4.8463917 ], dtype=float32)\n\n\n\nposterior_mean3 = jax.vmap(posterior_mean2)\nposterior_variance3 = jax.vmap(posterior_variance2)\n\n\nposterior_variance2(nu)\n\nArray(20.993286, dtype=float32)\n\n\n\n# gradient based computation\nnu = np.array([z, np.log(s)])\nposterior_mean2(nu), posterior_variance2(nu)\n\n(Array(1.8652706, dtype=float32), Array(20.993286, dtype=float32))\n\n\n\n# analytic computation\nnormalize = lambda x: np.exp(x - logsumexp(x))\ntau_grid = 1/sigma_grid**2\ngamma = 1/np.exp(nu[1])\nb = nu[0]\nposterior_variance_grid = 1/(gamma + tau_grid)\nposterior_pi = normalize(jnp.log(pi) + norm.logpdf(b, 0, np.sqrt(1/gamma + 1/tau_grid)))\nnp.sum(posterior_variance_grid * posterior_pi)\n\n/var/folders/0q/xlv6fxx51c17jf27j1ws0wx40000gn/T/ipykernel_97861/2498233326.py:3: RuntimeWarning: divide by zero encountered in divide\n  tau_grid = 1/sigma_grid**2\n\n\nnp.float64(7.633511455047038)\n\n\n\nposterior_variance_grid\n\narray([0.        , 0.90909091, 2.85714286, 4.73684211, 6.15384615,\n       7.14285714, 7.82608696, 8.30508475, 8.64864865, 8.9010989 ])\n\n\n\nX = np.random.normal(size=(100, 10))\nNu = np.random.normal(size=(10, 2))\nbeta = posterior_mean3(Nu)#, posterior_variance3(Nu)\ny = X @ beta + np.random.normal(size=100)\n\n\ndef q2mix(nu, pi, sigma_grid):\n    \"\"\"\n    get mixture representation for tilted scale mixture of gaussians\n    \"\"\"\n    \n\n\ndef kl(nu, pi, sigma_grid):\n    \ndef elbo(nu, y, X):\n    mu = posterior_mean3(nu)\n    var = posterior_variance(nu)\n    d = jnp.diag(X.T @ X)\n    -0.5 * jnp.sum((y - mu)**2) - 0.5 * jnp.sum(d * var) - kl(nu, pi, sigma_grid)"
  },
  {
    "objectID": "notebooks/penalty.html",
    "href": "notebooks/penalty.html",
    "title": "A fixed point iteration",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import partial\nfrom jax.scipy.interpolate import RegularGridInterpolator\n\n\ndef gaussian_mgf(eta, sigma=1):\n    return jnp.exp((sigma**2 * eta**2)/2)\n\ndef exponential_mgf(eta, rate=1):\n    return (rate / (rate - eta))\n\ndef laplace_mgf(eta, scale=1):\n    return (1 / (1 - (scale * eta)**2))\n    \ndef make_cdf(mgf, vmap=True):\n    def cdf(eta):\n        return jnp.log(mgf(eta))\n    cdf_prime = jax.grad(cdf)\n    if vmap:\n        return jax.jit(jax.vmap(cdf)), jax.jit(jax.vmap(cdf_prime))\n    else:\n        return jax.jit(cdf), jax.jit(cdf_prime)\n        \ndef make_point_mgf(mgf, pi):\n    def point_mgf(eta):\n        return (1 - pi) + pi * mgf(eta)\n    return point_mgf\n\ndef make_mixture_mgf(mgfs, pis):\n    def point_mgf(eta):\n        mgf_eval = jnp.array([f(eta) for f in mgfs])\n        return jnp.sum(pis * mgf_eval)\n    return point_mgf\n    \ndef make_penalty(kappa):\n    kappa_prime_inv = invert(jax.grad(kappa))\n    @jax.jit\n    @partial(jax.vmap, in_axes=(0))\n    def rho(theta):\n        return theta * kappa_prime_inv(theta) - kappa(kappa_prime_inv(theta))\n    return rho\n    \ndef point_cdf(eta, sigma=1, pi=0.1):\n    return pi * jax.grad(gaussian_mgf)(eta) / (1 - pi + pi * gaussian_mgf(eta))\n\n\\[\n\\nu_{g, \\sigma, t}(\\theta) = \\kappa^\\prime_g((t-\\theta)/\\sigma^2)) = \\theta\n\\]\nNotice that \\(\\nu(t) = 0\\) and \\(\\nu(0) = t\\) when \\(g\\) is a mean zero prior. Since it may alternate, I propose taking a running average of the iterates. I think a strategy like this should converge to a fixed point.\n\n# set up prior\nmgf = partial(gaussian_mgf, sigma=1)\nkappa, kappa_prime = make_cdf(mgf, vmap=False)\n\ndef make_shrinkage_op(kappa_prime):\n    def shrinkage_op(t, sigma2):\n        nu = lambda theta: kappa_prime((t - theta)/sigma2)\n\n        def scanf(x, it):\n            nux = nu(x)\n            xnew = (it-1)/it * x + (1/it) * nu(x)\n            return xnew, nux\n            \n        return jax.lax.scan(scanf, t/2, np.arange(2, 100))[0]\n    return shrinkage_op\n\n\nkappa_prime = lambda x: x\nS = jax.jit(jax.vmap(partial(make_shrinkage_op(kappa_prime), sigma2=1)))\nz = np.linspace(-10, 10, 100)\nplt.plot(z, S(z))\nplt.plot(z, z, c='k', linestyle='dotted')\n\n\n\n\n\n\n\n\n\n#plt.plot(z, kappa_prime(z))\n\nt = 1\nsigma2 = 0.5\nz = np.linspace(-0.9, 0.9, 100) * sigma2 + t\n\nnu = lambda theta: kappa_prime((t-theta)/sigma2)\nplt.plot(z, nu(z))\nplt.plot(z, z)\n\n\n\n\n\n\n\n\n\nS = partial(make_shrinkage_op(kappa_prime), sigma2=1)\nS(0.5)\n\nArray(0.33633628, dtype=float32)\n\n\n\nkappa_prime = lambda x: 2*x/(1-x**2)\nz = np.linspace(-0.99, 0.99, 100)\n#plt.plot(z, kappa_prime(z))\nS = jax.jit(jax.vmap(partial(make_shrinkage_op(kappa_prime), sigma2=1)))\nplt.plot(z, S(z))\nplt.plot(z, z, c='k', linestyle='dotted')\n\n\n\n\n\n\n\n\n\nkappa_prime = lambda theta: theta\nS = make_shrinkage_operator(kappa_prime)\nS(1., 1.)\n\n\n---------------------------------------------------------------------------\nTracerBoolConversionError                 Traceback (most recent call last)\nCell In[66], line 3\n      1 kappa_prime = lambda theta: theta\n      2 S = make_shrinkage_operator(kappa_prime)\n----&gt; 3 S(1., 1.)\n\nCell In[65], line 10, in make_shrinkage_operator.&lt;locals&gt;.shrinkage_op(t, sigma2)\n      8 def shrinkage_op(t, sigma2):\n      9     f = lambda theta: (kappa_prime((t - theta)/sigma2) - theta)\n---&gt; 10     theta =jax.lax.cond(\n     11         t &gt;= 0, \n     12         lambda: Bisection(optimality_fun=f, lower=0.9*t, upper=10*t).run().params,\n     13         lambda: Bisection(optimality_fun=f, lower=10*t, upper=0.9*t).run().params\n     14     )\n     15     return theta\n\n    [... skipping hidden 10 frame]\n\nCell In[65], line 12, in make_shrinkage_operator.&lt;locals&gt;.shrinkage_op.&lt;locals&gt;.&lt;lambda&gt;()\n      8 def shrinkage_op(t, sigma2):\n      9     f = lambda theta: (kappa_prime((t - theta)/sigma2) - theta)\n     10     theta =jax.lax.cond(\n     11         t &gt;= 0, \n---&gt; 12         lambda: Bisection(optimality_fun=f, lower=0.9*t, upper=10*t).run().params,\n     13         lambda: Bisection(optimality_fun=f, lower=10*t, upper=0.9*t).run().params\n     14     )\n     15     return theta\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/bisection.py:169, in Bisection.run(self, init_params, *args, **kwargs)\n    164 def run(self,\n    165         init_params: Optional[Any] = None,\n    166         *args,\n    167         **kwargs) -&gt; base.OptStep:\n    168   # We override run in order to set init_params=None by default.\n--&gt; 169   return super().run(init_params, *args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/base.py:358, in IterativeSolver.run(self, init_params, *args, **kwargs)\n    351   decorator = idf.custom_root(\n    352       self.optimality_fun,\n    353       has_aux=True,\n    354       solve=self.implicit_diff_solve,\n    355       reference_signature=reference_signature)\n    356   run = decorator(run)\n--&gt; 358 return run(init_params, *args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/implicit_diff.py:251, in _custom_root.&lt;locals&gt;.wrapped_solver_fun(*args, **kwargs)\n    249 args, kwargs = _signature_bind(solver_fun_signature, *args, **kwargs)\n    250 keys, vals = list(kwargs.keys()), list(kwargs.values())\n--&gt; 251 return make_custom_vjp_solver_fun(solver_fun, keys)(*args, *vals)\n\n    [... skipping hidden 11 frame]\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/implicit_diff.py:207, in _custom_root.&lt;locals&gt;.make_custom_vjp_solver_fun.&lt;locals&gt;.solver_fun_flat(*flat_args)\n    204 @jax.custom_vjp\n    205 def solver_fun_flat(*flat_args):\n    206   args, kwargs = _extract_kwargs(kwarg_keys, flat_args)\n--&gt; 207   return solver_fun(*args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/base.py:300, in IterativeSolver._run(self, init_params, *args, **kwargs)\n    296 def _run(self,\n    297          init_params: Any,\n    298          *args,\n    299          **kwargs) -&gt; OptStep:\n--&gt; 300   state = self.init_state(init_params, *args, **kwargs)\n    302   # We unroll the very first iteration. This allows `init_val` and `body_fun`\n    303   # below to have the same output type, which is a requirement of\n    304   # lax.while_loop and lax.scan.\n   (...)    315   # of a `lax.cond` for now in order to avoid staging the initial\n    316   # update and the run loop. They might not be staging compatible.\n    318   zero_step = self._make_zero_step(init_params, state)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/bisection.py:105, in Bisection.init_state(***failed resolving arguments***)\n     99 sign = jnp.where((lower_value &lt; 0) & (upper_value &gt;= 0),\n    100                  1,\n    101                  jnp.where((lower_value &gt; 0) & (upper_value &lt;= 0), -1, 0))\n    103 if self.check_bracket:\n    104   # Not jittable...\n--&gt; 105   if sign == 0:\n    106     raise ValueError(\"The root is not contained in [lower, upper]. \"\n    107                      \"`optimality_fun` evaluated at lower and upper should \"\n    108                      \"have opposite signs.\")\n    110 return BisectionState(iter_num=jnp.asarray(0),\n    111                       value=jnp.asarray(jnp.inf),\n    112                       error=jnp.asarray(jnp.inf),\n    113                       low=lower, high=upper,\n    114                       sign=jnp.asarray(sign),\n    115                       num_fun_eval=jnp.array(2, base.NUM_EVAL_DTYPE))\n\n    [... skipping hidden 1 frame]\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jax/_src/core.py:1604, in concretization_function_error.&lt;locals&gt;.error(self, arg)\n   1603 def error(self, arg):\n-&gt; 1604   raise TracerBoolConversionError(arg)\n\nTracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function solver_fun_flat at /Users/ktayeb/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/implicit_diff.py:204 for custom_vjp fun. This value became a tracer due to JAX operations on these lines:\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line /Users/ktayeb/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/bisection.py:90:12 (Bisection.init_state)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line /Users/ktayeb/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/bisection.py:91:12 (Bisection.init_state)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line /var/folders/0q/xlv6fxx51c17jf27j1ws0wx40000gn/T/ipykernel_77668/229207572.py:9:40 (make_shrinkage_operator.&lt;locals&gt;.shrinkage_op.&lt;locals&gt;.&lt;lambda&gt;)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line /var/folders/0q/xlv6fxx51c17jf27j1ws0wx40000gn/T/ipykernel_77668/229207572.py:9:39 (make_shrinkage_operator.&lt;locals&gt;.shrinkage_op.&lt;locals&gt;.&lt;lambda&gt;)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line /var/folders/0q/xlv6fxx51c17jf27j1ws0wx40000gn/T/ipykernel_77668/229207572.py:9:40 (make_shrinkage_operator.&lt;locals&gt;.shrinkage_op.&lt;locals&gt;.&lt;lambda&gt;)\n\n(Additional originating lines are not shown.)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError\n\n\n\n\nS(2., 1)\n\nArray(4.000001, dtype=float32)\n\n\n\nnp.linspace(0, 1, 10)\n\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])\n\n\n\nSv = jax.vmap(S, in_axes=(0, None))\nSv(np.linspace(0, 1, 10), 1.)\n\n\n---------------------------------------------------------------------------\nTracerBoolConversionError                 Traceback (most recent call last)\nCell In[55], line 2\n      1 Sv = jax.vmap(S, in_axes=(0, None))\n----&gt; 2 Sv(np.linspace(0, 1, 10), 1.)\n\n    [... skipping hidden 7 frame]\n\nCell In[47], line 11, in make_shrinkage_operator.&lt;locals&gt;.shrinkage_op(theta, sigma2)\n      9 lower, upper = get_range(jnp.array([theta*10, theta * 0.9]))\n     10 bisec = Bisection(optimality_fun=f, lower=lower, upper=upper)\n---&gt; 11 return bisec.run().params\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/bisection.py:169, in Bisection.run(self, init_params, *args, **kwargs)\n    164 def run(self,\n    165         init_params: Optional[Any] = None,\n    166         *args,\n    167         **kwargs) -&gt; base.OptStep:\n    168   # We override run in order to set init_params=None by default.\n--&gt; 169   return super().run(init_params, *args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/base.py:358, in IterativeSolver.run(self, init_params, *args, **kwargs)\n    351   decorator = idf.custom_root(\n    352       self.optimality_fun,\n    353       has_aux=True,\n    354       solve=self.implicit_diff_solve,\n    355       reference_signature=reference_signature)\n    356   run = decorator(run)\n--&gt; 358 return run(init_params, *args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/implicit_diff.py:251, in _custom_root.&lt;locals&gt;.wrapped_solver_fun(*args, **kwargs)\n    249 args, kwargs = _signature_bind(solver_fun_signature, *args, **kwargs)\n    250 keys, vals = list(kwargs.keys()), list(kwargs.values())\n--&gt; 251 return make_custom_vjp_solver_fun(solver_fun, keys)(*args, *vals)\n\n    [... skipping hidden 12 frame]\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/implicit_diff.py:207, in _custom_root.&lt;locals&gt;.make_custom_vjp_solver_fun.&lt;locals&gt;.solver_fun_flat(*flat_args)\n    204 @jax.custom_vjp\n    205 def solver_fun_flat(*flat_args):\n    206   args, kwargs = _extract_kwargs(kwarg_keys, flat_args)\n--&gt; 207   return solver_fun(*args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/base.py:300, in IterativeSolver._run(self, init_params, *args, **kwargs)\n    296 def _run(self,\n    297          init_params: Any,\n    298          *args,\n    299          **kwargs) -&gt; OptStep:\n--&gt; 300   state = self.init_state(init_params, *args, **kwargs)\n    302   # We unroll the very first iteration. This allows `init_val` and `body_fun`\n    303   # below to have the same output type, which is a requirement of\n    304   # lax.while_loop and lax.scan.\n   (...)    315   # of a `lax.cond` for now in order to avoid staging the initial\n    316   # update and the run loop. They might not be staging compatible.\n    318   zero_step = self._make_zero_step(init_params, state)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/bisection.py:105, in Bisection.init_state(***failed resolving arguments***)\n     99 sign = jnp.where((lower_value &lt; 0) & (upper_value &gt;= 0),\n    100                  1,\n    101                  jnp.where((lower_value &gt; 0) & (upper_value &lt;= 0), -1, 0))\n    103 if self.check_bracket:\n    104   # Not jittable...\n--&gt; 105   if sign == 0:\n    106     raise ValueError(\"The root is not contained in [lower, upper]. \"\n    107                      \"`optimality_fun` evaluated at lower and upper should \"\n    108                      \"have opposite signs.\")\n    110 return BisectionState(iter_num=jnp.asarray(0),\n    111                       value=jnp.asarray(jnp.inf),\n    112                       error=jnp.asarray(jnp.inf),\n    113                       low=lower, high=upper,\n    114                       sign=jnp.asarray(sign),\n    115                       num_fun_eval=jnp.array(2, base.NUM_EVAL_DTYPE))\n\n    [... skipping hidden 1 frame]\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jax/_src/core.py:1604, in concretization_function_error.&lt;locals&gt;.error(self, arg)\n   1603 def error(self, arg):\n-&gt; 1604   raise TracerBoolConversionError(arg)\n\nTracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[].\nThis BatchTracer with object id 4769498704 was created on line:\n  /Users/ktayeb/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/bisection.py:105:9 (Bisection.init_state)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError\n\n\n\n\nbisec = Bisection(optimality_fun=f, lower=-1, upper=2)\nprint(bisec.run().params)\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jax/_src/api.py:1063, in _mapped_axis_size.&lt;locals&gt;._get_axis_size(name, shape, axis)\n   1062 try:\n-&gt; 1063   return shape[axis]\n   1064 except (IndexError, TypeError) as e:\n\nIndexError: tuple index out of range\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nCell In[17], line 2\n      1 bisec = Bisection(optimality_fun=f, lower=-1, upper=2)\n----&gt; 2 print(bisec.run().params)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/bisection.py:169, in Bisection.run(self, init_params, *args, **kwargs)\n    164 def run(self,\n    165         init_params: Optional[Any] = None,\n    166         *args,\n    167         **kwargs) -&gt; base.OptStep:\n    168   # We override run in order to set init_params=None by default.\n--&gt; 169   return super().run(init_params, *args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/base.py:358, in IterativeSolver.run(self, init_params, *args, **kwargs)\n    351   decorator = idf.custom_root(\n    352       self.optimality_fun,\n    353       has_aux=True,\n    354       solve=self.implicit_diff_solve,\n    355       reference_signature=reference_signature)\n    356   run = decorator(run)\n--&gt; 358 return run(init_params, *args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/implicit_diff.py:251, in _custom_root.&lt;locals&gt;.wrapped_solver_fun(*args, **kwargs)\n    249 args, kwargs = _signature_bind(solver_fun_signature, *args, **kwargs)\n    250 keys, vals = list(kwargs.keys()), list(kwargs.values())\n--&gt; 251 return make_custom_vjp_solver_fun(solver_fun, keys)(*args, *vals)\n\n    [... skipping hidden 9 frame]\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/implicit_diff.py:207, in _custom_root.&lt;locals&gt;.make_custom_vjp_solver_fun.&lt;locals&gt;.solver_fun_flat(*flat_args)\n    204 @jax.custom_vjp\n    205 def solver_fun_flat(*flat_args):\n    206   args, kwargs = _extract_kwargs(kwarg_keys, flat_args)\n--&gt; 207   return solver_fun(*args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/base.py:300, in IterativeSolver._run(self, init_params, *args, **kwargs)\n    296 def _run(self,\n    297          init_params: Any,\n    298          *args,\n    299          **kwargs) -&gt; OptStep:\n--&gt; 300   state = self.init_state(init_params, *args, **kwargs)\n    302   # We unroll the very first iteration. This allows `init_val` and `body_fun`\n    303   # below to have the same output type, which is a requirement of\n    304   # lax.while_loop and lax.scan.\n   (...)    315   # of a `lax.cond` for now in order to avoid staging the initial\n    316   # update and the run loop. They might not be staging compatible.\n    318   zero_step = self._make_zero_step(init_params, state)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/bisection.py:93, in Bisection.init_state(***failed resolving arguments***)\n     90 lower = jnp.asarray(self.lower, float)\n     91 upper = jnp.asarray(self.upper, float)\n---&gt; 93 lower_value = self.optimality_fun(lower, *args, **kwargs)\n     94 upper_value = self.optimality_fun(upper, *args, **kwargs)\n     96 # sign = 1: the function is increasing\n     97 # sign = -1: the function is decreasing\n     98 # sign = 0: the root is not contained in [lower, upper]\n\nCell In[10], line 4, in &lt;lambda&gt;(t)\n      1 theta = 1.\n      2 sigma2 = 1.\n----&gt; 4 f = lambda t: (kappa_prime((t - theta)/sigma2) - theta)\n      5 f(np.array([1., 4.]))\n\n    [... skipping hidden 19 frame]\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jax/_src/api.py:1067, in _mapped_axis_size.&lt;locals&gt;._get_axis_size(name, shape, axis)\n   1065 min_rank = axis + 1 if axis &gt;= 0 else -axis\n   1066 # TODO(mattjj): better error message here\n-&gt; 1067 raise ValueError(\n   1068     f\"{name} was requested to map its argument along axis {axis}, \"\n   1069     f\"which implies that its rank should be at least {min_rank}, \"\n   1070     f\"but is only {len(shape)} (its shape is {shape})\") from e\n\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\n\n\n\n\nF(np.arange(10))\n\narray([ -2,  -2,   4,  22,  58, 118, 208, 334, 502, 718])\n\n\n\ntheta = 1.\nsigma2 = 1.\n\nkappa_prime = lambda theta: theta\nf = lambda t: (kappa_prime((t - theta)/sigma2) - theta)\nbisec = Bisection(optimality_fun=f, lower=1, upper=4)\nbisec.run().params\n\nArray(2.0000076, dtype=float32)\n\n\n\nfrom jaxopt import Bisection\nf = lambda t: (kappa_prime((t - theta)/sigma2) - theta)[None]\nbisec = Bisection(optimality_fun=f, lower=1, upper=4)\nbisec.run().\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jax/_src/api.py:1063, in _mapped_axis_size.&lt;locals&gt;._get_axis_size(name, shape, axis)\n   1062 try:\n-&gt; 1063   return shape[axis]\n   1064 except (IndexError, TypeError) as e:\n\nIndexError: tuple index out of range\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nCell In[109], line 4\n      2 f = lambda t: (kappa_prime((t - theta)/sigma2) - theta)[None]\n      3 bisec = Bisection(optimality_fun=f, lower=1, upper=4)\n----&gt; 4 bisec.run()\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/bisection.py:169, in Bisection.run(self, init_params, *args, **kwargs)\n    164 def run(self,\n    165         init_params: Optional[Any] = None,\n    166         *args,\n    167         **kwargs) -&gt; base.OptStep:\n    168   # We override run in order to set init_params=None by default.\n--&gt; 169   return super().run(init_params, *args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/base.py:358, in IterativeSolver.run(self, init_params, *args, **kwargs)\n    351   decorator = idf.custom_root(\n    352       self.optimality_fun,\n    353       has_aux=True,\n    354       solve=self.implicit_diff_solve,\n    355       reference_signature=reference_signature)\n    356   run = decorator(run)\n--&gt; 358 return run(init_params, *args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/implicit_diff.py:251, in _custom_root.&lt;locals&gt;.wrapped_solver_fun(*args, **kwargs)\n    249 args, kwargs = _signature_bind(solver_fun_signature, *args, **kwargs)\n    250 keys, vals = list(kwargs.keys()), list(kwargs.values())\n--&gt; 251 return make_custom_vjp_solver_fun(solver_fun, keys)(*args, *vals)\n\n    [... skipping hidden 9 frame]\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/implicit_diff.py:207, in _custom_root.&lt;locals&gt;.make_custom_vjp_solver_fun.&lt;locals&gt;.solver_fun_flat(*flat_args)\n    204 @jax.custom_vjp\n    205 def solver_fun_flat(*flat_args):\n    206   args, kwargs = _extract_kwargs(kwarg_keys, flat_args)\n--&gt; 207   return solver_fun(*args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/base.py:300, in IterativeSolver._run(self, init_params, *args, **kwargs)\n    296 def _run(self,\n    297          init_params: Any,\n    298          *args,\n    299          **kwargs) -&gt; OptStep:\n--&gt; 300   state = self.init_state(init_params, *args, **kwargs)\n    302   # We unroll the very first iteration. This allows `init_val` and `body_fun`\n    303   # below to have the same output type, which is a requirement of\n    304   # lax.while_loop and lax.scan.\n   (...)    315   # of a `lax.cond` for now in order to avoid staging the initial\n    316   # update and the run loop. They might not be staging compatible.\n    318   zero_step = self._make_zero_step(init_params, state)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/bisection.py:93, in Bisection.init_state(***failed resolving arguments***)\n     90 lower = jnp.asarray(self.lower, float)\n     91 upper = jnp.asarray(self.upper, float)\n---&gt; 93 lower_value = self.optimality_fun(lower, *args, **kwargs)\n     94 upper_value = self.optimality_fun(upper, *args, **kwargs)\n     96 # sign = 1: the function is increasing\n     97 # sign = -1: the function is decreasing\n     98 # sign = 0: the root is not contained in [lower, upper]\n\nCell In[109], line 2, in &lt;lambda&gt;(t)\n      1 from jaxopt import Bisection\n----&gt; 2 f = lambda t: (kappa_prime((t - theta)/sigma2) - theta)[None]\n      3 bisec = Bisection(optimality_fun=f, lower=1, upper=4)\n      4 bisec.run()\n\n    [... skipping hidden 19 frame]\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jax/_src/api.py:1067, in _mapped_axis_size.&lt;locals&gt;._get_axis_size(name, shape, axis)\n   1065 min_rank = axis + 1 if axis &gt;= 0 else -axis\n   1066 # TODO(mattjj): better error message here\n-&gt; 1067 raise ValueError(\n   1068     f\"{name} was requested to map its argument along axis {axis}, \"\n   1069     f\"which implies that its rank should be at least {min_rank}, \"\n   1070     f\"but is only {len(shape)} (its shape is {shape})\") from e\n\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\n\n\n\n\nsolver = ScipyRootFinding(optimality_fun=f)\nsolver.run(np.ones(1))\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[91], line 2\n      1 solver = ScipyRootFinding(optimality_fun=f)\n----&gt; 2 solver.run(np.ones(1))\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/implicit_diff.py:251, in _custom_root.&lt;locals&gt;.wrapped_solver_fun(*args, **kwargs)\n    249 args, kwargs = _signature_bind(solver_fun_signature, *args, **kwargs)\n    250 keys, vals = list(kwargs.keys()), list(kwargs.values())\n--&gt; 251 return make_custom_vjp_solver_fun(solver_fun, keys)(*args, *vals)\n\n    [... skipping hidden 9 frame]\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/implicit_diff.py:207, in _custom_root.&lt;locals&gt;.make_custom_vjp_solver_fun.&lt;locals&gt;.solver_fun_flat(*flat_args)\n    204 @jax.custom_vjp\n    205 def solver_fun_flat(*flat_args):\n    206   args, kwargs = _extract_kwargs(kwarg_keys, flat_args)\n--&gt; 207   return solver_fun(*args, **kwargs)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jaxopt/_src/scipy_wrappers.py:532, in ScipyRootFinding.run(self, init_params, *args, **kwargs)\n    527   return jnp_to_onp(value_jnp, self.dtype), jac_jnp_to_onp(jacs_jnp)\n    529 # Argument `args` is unused but must be not None to ensure that some sanity checks are performed\n    530 # correctly in Scipy for optimizers that don't use the Jacobian (such as Broyden).\n    531 # See the related issue: https://github.com/google/jaxopt/issues/290\n--&gt; 532 res = osp.optimize.root(scipy_fun, jnp_to_onp(init_params, self.dtype),\n    533                         args=(None,),\n    534                         jac=True,\n    535                         tol=self.tol,\n    536                         method=self.method,\n    537                         options=self.options)\n    539 params = tree_util.tree_map(jnp.asarray, onp_to_jnp(res.x))\n    541 # NOTE: maybe there is a better way to do the following (zramzi)\n\nFile ~/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/scipy/optimize/_root.py:222, in root(fun, x0, args, method, jac, tol, callback, options)\n    219 if not isinstance(args, tuple):\n    220     args = (args,)\n--&gt; 222 meth = method.lower()\n    223 if options is None:\n    224     options = {}\n\nAttributeError: 'NoneType' object has no attribute 'lower'\n\n\n\n\nz[np.abs(f(z)).argmin()]\n\nnp.float64(-0.19919919919919926)\n\n\n\nplt.plot(z, kappa_prime(z))\n\n\n\n\n\n\n\n\n\nz = np.linspace(-1, 1, 100) * 2\nkappa_prime(z)\n\nArray([-2.        , -1.9595959 , -1.919192  , -1.8787879 , -1.8383838 ,\n       -1.7979798 , -1.7575758 , -1.7171717 , -1.6767677 , -1.6363636 ,\n       -1.5959595 , -1.5555556 , -1.5151515 , -1.4747474 , -1.4343435 ,\n       -1.3939394 , -1.3535353 , -1.3131313 , -1.2727273 , -1.2323233 ,\n       -1.1919192 , -1.1515151 , -1.1111112 , -1.0707071 , -1.030303  ,\n       -0.989899  , -0.94949496, -0.90909094, -0.86868685, -0.82828283,\n       -0.7878788 , -0.74747473, -0.7070707 , -0.6666667 , -0.6262626 ,\n       -0.5858586 , -0.54545456, -0.5050505 , -0.46464646, -0.42424244,\n       -0.3838384 , -0.34343433, -0.3030303 , -0.26262626, -0.22222222,\n       -0.18181819, -0.14141414, -0.1010101 , -0.06060606, -0.02020202,\n        0.02020202,  0.06060606,  0.1010101 ,  0.14141414,  0.18181819,\n        0.22222222,  0.26262626,  0.3030303 ,  0.34343433,  0.3838384 ,\n        0.42424244,  0.46464646,  0.5050505 ,  0.54545456,  0.5858586 ,\n        0.6262626 ,  0.6666667 ,  0.7070707 ,  0.74747473,  0.7878788 ,\n        0.82828283,  0.86868685,  0.90909094,  0.94949496,  0.989899  ,\n        1.030303  ,  1.0707071 ,  1.1111112 ,  1.1515151 ,  1.1919192 ,\n        1.2323233 ,  1.2727273 ,  1.3131313 ,  1.3535353 ,  1.3939394 ,\n        1.4343435 ,  1.4747474 ,  1.5151515 ,  1.5555556 ,  1.5959595 ,\n        1.6363636 ,  1.6767677 ,  1.7171717 ,  1.7575758 ,  1.7979798 ,\n        1.8383838 ,  1.8787879 ,  1.919192  ,  1.9595959 ,  2.        ],      dtype=float32)\n\n\n\n#\nmgf = partial(gaussian_mgf, sigma=1)\nkappa, kappa_prime = make_cdf(mgf)\nz = np.linspace(-1, 1, 100) * 2\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\nplt.plot(theta, penalty, label=r'$\\pi=1$')\n\n#\nmgf = partial(gaussian_mgf, sigma=1)\npoint_mgf = make_point_mgf(mgf, pi=0.1)\nkappa, kappa_prime = make_cdf(point_mgf)\nz = np.linspace(-1, 1, 100) * 2.5\n\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\nplt.plot(theta, penalty, label=r'$\\pi=0.1$')\nplt.legend()\nplt.title('Point-Normal')\nplt.xlabel('Posterior mean')\nplt.ylabel('Penalty')\n\nText(0, 0.5, 'Penalty')\n\n\n\n\n\n\n\n\n\n\n#\nmgf = partial(exponential_mgf, rate=1)\nkappa, kappa_prime = make_cdf(mgf)\nz = np.linspace(-5, 0.5, 100)\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\nplt.plot(theta, penalty, label=r'$\\pi=1$')\n\n#\nmgf = partial(exponential_mgf, rate=1)\npoint_mgf = make_point_mgf(mgf, pi=0.1)\nkappa, kappa_prime = make_cdf(point_mgf)\nz = np.linspace(-5, 0.8, 100)\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\nplt.plot(theta, penalty, label=r'$\\pi=0.1$')\nplt.legend()\nplt.title('Point-Exponential')\nplt.xlabel('Posterior mean')\nplt.ylabel('Penalty')\n\nText(0, 0.5, 'Penalty')\n\n\n\n\n\n\n\n\n\n\nz = np.arcsin(np.linspace(0, np.pi, 1000))\n\n#\nmgf = partial(laplace_mgf, scale=1)\nkappa, kappa_prime = make_cdf(mgf)\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\nplt.plot(theta, penalty, label=r'$\\pi=1$')\n\n#\nmgf = partial(laplace_mgf, scale=1)\npoint_mgf = make_point_mgf(mgf, pi=0.1)\nkappa, kappa_prime = make_cdf(point_mgf)\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\n\n\nmgf = partial(laplace_mgf, scale=1)\npoint_mgf = make_point_mgf(mgf, pi=0.01)\nkappa, kappa_prime = make_cdf(point_mgf)\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\nplt.plot(theta, penalty, label=r'$\\pi=0.01$')\n\nmgf = partial(laplace_mgf, scale=1)\npoint_mgf = make_point_mgf(mgf, pi=0.001)\nkappa, kappa_prime = make_cdf(point_mgf)\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\nplt.plot(theta, penalty, label=r'$\\pi=0.001$')\n\nplt.legend()\nplt.title('Point-Laplace')\nplt.xlabel('Posterior mean')\nplt.ylabel('Penalty')\n\n/var/folders/0q/xlv6fxx51c17jf27j1ws0wx40000gn/T/ipykernel_63174/650912464.py:1: RuntimeWarning: invalid value encountered in arcsin\n  z = np.arcsin(np.linspace(0, np.pi, 1000))\n\n\nText(0, 0.5, 'Penalty')\n\n\n\n\n\n\n\n\n\n\nz = np.cos(np.linspace(0, np.pi, 1000))\nz = z[20:-20]\n#\nmgf = partial(laplace_mgf, scale=1)\nkappa, kappa_prime = make_cdf(mgf)\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\nplt.plot(z, theta, label=r'$\\pi=1$')\n\n#\nmgf = partial(laplace_mgf, scale=1)\npoint_mgf = make_point_mgf(mgf, pi=0.1)\nkappa, kappa_prime = make_cdf(point_mgf)\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\n\n\nmgf = partial(laplace_mgf, scale=1)\npoint_mgf = make_point_mgf(mgf, pi=0.01)\nkappa, kappa_prime = make_cdf(point_mgf)\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\nplt.plot(z, theta, label=r'$\\pi=0.01$')\n\nmgf = partial(laplace_mgf, scale=1)\npoint_mgf = make_point_mgf(mgf, pi=0.001)\nkappa, kappa_prime = make_cdf(point_mgf)\ntheta = kappa_prime(z)\npenalty = theta * z - kappa(z)\nplt.plot(z, theta, label=r'$\\pi=0.001$')\n\nplt.legend()\nplt.title('Point-Laplace')\nplt.xlabel('Posterior mean')\nplt.ylabel('Penalty')\n\nText(0, 0.5, 'Penalty')\n\n\n\n\n\n\n\n\n\n\nz\n\narray([ 1.        ,  0.99999506,  0.99998022,  0.9999555 ,  0.99992089,\n        0.99987639,  0.999822  ,  0.99975772,  0.99968356,  0.99959951,\n        0.99950557,  0.99940175,  0.99928805,  0.99916446,  0.999031  ,\n        0.99888765,  0.99873443,  0.99857133,  0.99839835,  0.9982155 ,\n        0.99802278,  0.99782019,  0.99760773,  0.9973854 ,  0.99715321,\n        0.99691116,  0.99665925,  0.99639749,  0.99612587,  0.9958444 ,\n        0.99555308,  0.99525192,  0.99494091,  0.99462007,  0.99428939,\n        0.99394887,  0.99359853,  0.99323836,  0.99286837,  0.99248855,\n        0.99209893,  0.99169949,  0.99129025,  0.9908712 ,  0.99044235,\n        0.99000371,  0.98955528,  0.98909706,  0.98862906,  0.98815128,\n        0.98766373,  0.98716641,  0.98665934,  0.9861425 ,  0.98561591,\n        0.98507957,  0.9845335 ,  0.98397768,  0.98341214,  0.98283687,\n        0.98225188,  0.98165717,  0.98105276,  0.98043865,  0.97981484,\n        0.97918134,  0.97853816,  0.9778853 ,  0.97722277,  0.97655057,\n        0.97586872,  0.97517722,  0.97447607,  0.97376528,  0.97304487,\n        0.97231483,  0.97157518,  0.97082592,  0.97006706,  0.96929861,\n        0.96852057,  0.96773295,  0.96693576,  0.96612901,  0.9653127 ,\n        0.96448685,  0.96365146,  0.96280654,  0.9619521 ,  0.96108814,\n        0.96021469,  0.95933173,  0.95843929,  0.95753737,  0.95662598,\n        0.95570513,  0.95477483,  0.95383508,  0.95288591,  0.95192731,\n        0.95095929,  0.94998187,  0.94899506,  0.94799886,  0.94699329,\n        0.94597835,  0.94495406,  0.94392042,  0.94287745,  0.94182515,\n        0.94076354,  0.93969262,  0.93861241,  0.93752292,  0.93642416,\n        0.93531614,  0.93419886,  0.93307235,  0.93193662,  0.93079166,\n        0.9296375 ,  0.92847415,  0.92730161,  0.92611991,  0.92492904,\n        0.92372903,  0.92251988,  0.92130162,  0.92007423,  0.91883775,\n        0.91759219,  0.91633755,  0.91507385,  0.91380109,  0.9125193 ,\n        0.91122849,  0.90992867,  0.90861984,  0.90730203,  0.90597525,\n        0.90463951,  0.90329482,  0.9019412 ,  0.90057866,  0.89920721,\n        0.89782688,  0.89643766,  0.89503957,  0.89363264,  0.89221687,\n        0.89079227,  0.88935887,  0.88791667,  0.88646569,  0.88500594,\n        0.88353744,  0.8820602 ,  0.88057424,  0.87907957,  0.87757621,\n        0.87606417,  0.87454347,  0.87301411,  0.87147613,  0.86992952,\n        0.86837431,  0.86681052,  0.86523815,  0.86365722,  0.86206776,\n        0.86046977,  0.85886327,  0.85724827,  0.8556248 ,  0.85399287,\n        0.85235249,  0.85070368,  0.84904646,  0.84738084,  0.84570685,\n        0.84402448,  0.84233378,  0.84063474,  0.83892739,  0.83721174,\n        0.83548781,  0.83375562,  0.83201519,  0.83026652,  0.82850965,\n        0.82674458,  0.82497134,  0.82318994,  0.82140039,  0.81960273,\n        0.81779696,  0.8159831 ,  0.81416117,  0.81233119,  0.81049318,\n        0.80864715,  0.80679312,  0.80493112,  0.80306115,  0.80118325,\n        0.79929742,  0.79740368,  0.79550207,  0.79359258,  0.79167524,\n        0.78975008,  0.78781711,  0.78587634,  0.7839278 ,  0.78197151,\n        0.78000749,  0.77803575,  0.77605632,  0.77406922,  0.77207446,\n        0.77007206,  0.76806205,  0.76604444,  0.76401926,  0.76198652,\n        0.75994625,  0.75789846,  0.75584317,  0.75378041,  0.7517102 ,\n        0.74963255,  0.74754749,  0.74545504,  0.74335521,  0.74124804,\n        0.73913353,  0.73701171,  0.7348826 ,  0.73274623,  0.73060261,\n        0.72845177,  0.72629372,  0.72412849,  0.72195609,  0.71977656,\n        0.71758991,  0.71539616,  0.71319534,  0.71098747,  0.70877256,\n        0.70655065,  0.70432174,  0.70208588,  0.69984306,  0.69759333,\n        0.6953367 ,  0.69307319,  0.69080283,  0.68852564,  0.68624164,\n        0.68395085,  0.6816533 ,  0.679349  ,  0.67703799,  0.67472028,\n        0.6723959 ,  0.67006487,  0.66772722,  0.66538296,  0.66303212,\n        0.66067472,  0.65831079,  0.65594035,  0.65356343,  0.65118003,\n        0.6487902 ,  0.64639396,  0.64399132,  0.64158231,  0.63916696,\n        0.63674529,  0.63431732,  0.63188307,  0.62944258,  0.62699586,\n        0.62454294,  0.62208385,  0.6196186 ,  0.61714723,  0.61466975,\n        0.6121862 ,  0.60969659,  0.60720095,  0.60469931,  0.60219168,\n        0.5996781 ,  0.59715859,  0.59463318,  0.59210188,  0.58956473,\n        0.58702175,  0.58447296,  0.58191839,  0.57935807,  0.57679202,\n        0.57422026,  0.57164283,  0.56905974,  0.56647103,  0.56387671,\n        0.56127682,  0.55867137,  0.5560604 ,  0.55344393,  0.55082199,\n        0.5481946 ,  0.54556179,  0.54292359,  0.54028001,  0.5376311 ,\n        0.53497686,  0.53231734,  0.52965255,  0.52698252,  0.52430728,\n        0.52162686,  0.51894128,  0.51625056,  0.51355474,  0.51085385,\n        0.5081479 ,  0.50543692,  0.50272095,  0.5       ,  0.49727411,\n        0.4945433 ,  0.4918076 ,  0.48906704,  0.48632164,  0.48357143,\n        0.48081644,  0.4780567 ,  0.47529222,  0.47252305,  0.4697492 ,\n        0.46697071,  0.4641876 ,  0.4613999 ,  0.45860764,  0.45581084,\n        0.45300953,  0.45020374,  0.44739351,  0.44457884,  0.44175978,\n        0.43893635,  0.43610858,  0.4332765 ,  0.43044014,  0.42759951,\n        0.42475466,  0.42190561,  0.41905238,  0.41619501,  0.41333353,\n        0.41046795,  0.40759832,  0.40472466,  0.40184699,  0.39896535,\n        0.39607977,  0.39319026,  0.39029687,  0.38739962,  0.38449854,\n        0.38159366,  0.378685  ,  0.3757726 ,  0.37285648,  0.36993667,\n        0.36701321,  0.36408611,  0.36115542,  0.35822115,  0.35528334,\n        0.35234202,  0.34939721,  0.34644895,  0.34349726,  0.34054218,\n        0.33758372,  0.33462193,  0.33165683,  0.32868845,  0.32571682,\n        0.32274197,  0.31976392,  0.31678272,  0.31379838,  0.31081094,\n        0.30782042,  0.30482686,  0.30183029,  0.29883073,  0.29582821,\n        0.29282277,  0.28981444,  0.28680323,  0.28378919,  0.28077235,\n        0.27775273,  0.27473036,  0.27170527,  0.2686775 ,  0.26564707,\n        0.26261401,  0.25957836,  0.25654014,  0.25349938,  0.25045612,\n        0.24741038,  0.24436219,  0.24131158,  0.23825859,  0.23520324,\n        0.23214557,  0.2290856 ,  0.22602337,  0.2229589 ,  0.21989222,\n        0.21682337,  0.21375238,  0.21067927,  0.20760408,  0.20452683,\n        0.20144757,  0.19836631,  0.19528309,  0.19219794,  0.18911088,\n        0.18602196,  0.1829312 ,  0.17983863,  0.17674428,  0.17364818,\n        0.17055036,  0.16745086,  0.1643497 ,  0.16124692,  0.15814254,\n        0.1550366 ,  0.15192912,  0.14882015,  0.1457097 ,  0.14259781,\n        0.13948451,  0.13636983,  0.1332538 ,  0.13013645,  0.12701782,\n        0.12389793,  0.12077682,  0.11765451,  0.11453103,  0.11140643,\n        0.10828072,  0.10515394,  0.10202613,  0.0988973 ,  0.0957675 ,\n        0.09263674,  0.08950508,  0.08637252,  0.08323912,  0.08010489,\n        0.07696986,  0.07383408,  0.07069757,  0.06756035,  0.06442247,\n        0.06128395,  0.05814483,  0.05500513,  0.05186489,  0.04872413,\n        0.04558289,  0.0424412 ,  0.03929909,  0.0361566 ,  0.03301374,\n        0.02987056,  0.02672708,  0.02358334,  0.02043937,  0.01729519,\n        0.01415085,  0.01100636,  0.00786176,  0.00471709,  0.00157237,\n       -0.00157237, -0.00471709, -0.00786176, -0.01100636, -0.01415085,\n       -0.01729519, -0.02043937, -0.02358334, -0.02672708, -0.02987056,\n       -0.03301374, -0.0361566 , -0.03929909, -0.0424412 , -0.04558289,\n       -0.04872413, -0.05186489, -0.05500513, -0.05814483, -0.06128395,\n       -0.06442247, -0.06756035, -0.07069757, -0.07383408, -0.07696986,\n       -0.08010489, -0.08323912, -0.08637252, -0.08950508, -0.09263674,\n       -0.0957675 , -0.0988973 , -0.10202613, -0.10515394, -0.10828072,\n       -0.11140643, -0.11453103, -0.11765451, -0.12077682, -0.12389793,\n       -0.12701782, -0.13013645, -0.1332538 , -0.13636983, -0.13948451,\n       -0.14259781, -0.1457097 , -0.14882015, -0.15192912, -0.1550366 ,\n       -0.15814254, -0.16124692, -0.1643497 , -0.16745086, -0.17055036,\n       -0.17364818, -0.17674428, -0.17983863, -0.1829312 , -0.18602196,\n       -0.18911088, -0.19219794, -0.19528309, -0.19836631, -0.20144757,\n       -0.20452683, -0.20760408, -0.21067927, -0.21375238, -0.21682337,\n       -0.21989222, -0.2229589 , -0.22602337, -0.2290856 , -0.23214557,\n       -0.23520324, -0.23825859, -0.24131158, -0.24436219, -0.24741038,\n       -0.25045612, -0.25349938, -0.25654014, -0.25957836, -0.26261401,\n       -0.26564707, -0.2686775 , -0.27170527, -0.27473036, -0.27775273,\n       -0.28077235, -0.28378919, -0.28680323, -0.28981444, -0.29282277,\n       -0.29582821, -0.29883073, -0.30183029, -0.30482686, -0.30782042,\n       -0.31081094, -0.31379838, -0.31678272, -0.31976392, -0.32274197,\n       -0.32571682, -0.32868845, -0.33165683, -0.33462193, -0.33758372,\n       -0.34054218, -0.34349726, -0.34644895, -0.34939721, -0.35234202,\n       -0.35528334, -0.35822115, -0.36115542, -0.36408611, -0.36701321,\n       -0.36993667, -0.37285648, -0.3757726 , -0.378685  , -0.38159366,\n       -0.38449854, -0.38739962, -0.39029687, -0.39319026, -0.39607977,\n       -0.39896535, -0.40184699, -0.40472466, -0.40759832, -0.41046795,\n       -0.41333353, -0.41619501, -0.41905238, -0.42190561, -0.42475466,\n       -0.42759951, -0.43044014, -0.4332765 , -0.43610858, -0.43893635,\n       -0.44175978, -0.44457884, -0.44739351, -0.45020374, -0.45300953,\n       -0.45581084, -0.45860764, -0.4613999 , -0.4641876 , -0.46697071,\n       -0.4697492 , -0.47252305, -0.47529222, -0.4780567 , -0.48081644,\n       -0.48357143, -0.48632164, -0.48906704, -0.4918076 , -0.4945433 ,\n       -0.49727411, -0.5       , -0.50272095, -0.50543692, -0.5081479 ,\n       -0.51085385, -0.51355474, -0.51625056, -0.51894128, -0.52162686,\n       -0.52430728, -0.52698252, -0.52965255, -0.53231734, -0.53497686,\n       -0.5376311 , -0.54028001, -0.54292359, -0.54556179, -0.5481946 ,\n       -0.55082199, -0.55344393, -0.5560604 , -0.55867137, -0.56127682,\n       -0.56387671, -0.56647103, -0.56905974, -0.57164283, -0.57422026,\n       -0.57679202, -0.57935807, -0.58191839, -0.58447296, -0.58702175,\n       -0.58956473, -0.59210188, -0.59463318, -0.59715859, -0.5996781 ,\n       -0.60219168, -0.60469931, -0.60720095, -0.60969659, -0.6121862 ,\n       -0.61466975, -0.61714723, -0.6196186 , -0.62208385, -0.62454294,\n       -0.62699586, -0.62944258, -0.63188307, -0.63431732, -0.63674529,\n       -0.63916696, -0.64158231, -0.64399132, -0.64639396, -0.6487902 ,\n       -0.65118003, -0.65356343, -0.65594035, -0.65831079, -0.66067472,\n       -0.66303212, -0.66538296, -0.66772722, -0.67006487, -0.6723959 ,\n       -0.67472028, -0.67703799, -0.679349  , -0.6816533 , -0.68395085,\n       -0.68624164, -0.68852564, -0.69080283, -0.69307319, -0.6953367 ,\n       -0.69759333, -0.69984306, -0.70208588, -0.70432174, -0.70655065,\n       -0.70877256, -0.71098747, -0.71319534, -0.71539616, -0.71758991,\n       -0.71977656, -0.72195609, -0.72412849, -0.72629372, -0.72845177,\n       -0.73060261, -0.73274623, -0.7348826 , -0.73701171, -0.73913353,\n       -0.74124804, -0.74335521, -0.74545504, -0.74754749, -0.74963255,\n       -0.7517102 , -0.75378041, -0.75584317, -0.75789846, -0.75994625,\n       -0.76198652, -0.76401926, -0.76604444, -0.76806205, -0.77007206,\n       -0.77207446, -0.77406922, -0.77605632, -0.77803575, -0.78000749,\n       -0.78197151, -0.7839278 , -0.78587634, -0.78781711, -0.78975008,\n       -0.79167524, -0.79359258, -0.79550207, -0.79740368, -0.79929742,\n       -0.80118325, -0.80306115, -0.80493112, -0.80679312, -0.80864715,\n       -0.81049318, -0.81233119, -0.81416117, -0.8159831 , -0.81779696,\n       -0.81960273, -0.82140039, -0.82318994, -0.82497134, -0.82674458,\n       -0.82850965, -0.83026652, -0.83201519, -0.83375562, -0.83548781,\n       -0.83721174, -0.83892739, -0.84063474, -0.84233378, -0.84402448,\n       -0.84570685, -0.84738084, -0.84904646, -0.85070368, -0.85235249,\n       -0.85399287, -0.8556248 , -0.85724827, -0.85886327, -0.86046977,\n       -0.86206776, -0.86365722, -0.86523815, -0.86681052, -0.86837431,\n       -0.86992952, -0.87147613, -0.87301411, -0.87454347, -0.87606417,\n       -0.87757621, -0.87907957, -0.88057424, -0.8820602 , -0.88353744,\n       -0.88500594, -0.88646569, -0.88791667, -0.88935887, -0.89079227,\n       -0.89221687, -0.89363264, -0.89503957, -0.89643766, -0.89782688,\n       -0.89920721, -0.90057866, -0.9019412 , -0.90329482, -0.90463951,\n       -0.90597525, -0.90730203, -0.90861984, -0.90992867, -0.91122849,\n       -0.9125193 , -0.91380109, -0.91507385, -0.91633755, -0.91759219,\n       -0.91883775, -0.92007423, -0.92130162, -0.92251988, -0.92372903,\n       -0.92492904, -0.92611991, -0.92730161, -0.92847415, -0.9296375 ,\n       -0.93079166, -0.93193662, -0.93307235, -0.93419886, -0.93531614,\n       -0.93642416, -0.93752292, -0.93861241, -0.93969262, -0.94076354,\n       -0.94182515, -0.94287745, -0.94392042, -0.94495406, -0.94597835,\n       -0.94699329, -0.94799886, -0.94899506, -0.94998187, -0.95095929,\n       -0.95192731, -0.95288591, -0.95383508, -0.95477483, -0.95570513,\n       -0.95662598, -0.95753737, -0.95843929, -0.95933173, -0.96021469,\n       -0.96108814, -0.9619521 , -0.96280654, -0.96365146, -0.96448685,\n       -0.9653127 , -0.96612901, -0.96693576, -0.96773295, -0.96852057,\n       -0.96929861, -0.97006706, -0.97082592, -0.97157518, -0.97231483,\n       -0.97304487, -0.97376528, -0.97447607, -0.97517722, -0.97586872,\n       -0.97655057, -0.97722277, -0.9778853 , -0.97853816, -0.97918134,\n       -0.97981484, -0.98043865, -0.98105276, -0.98165717, -0.98225188,\n       -0.98283687, -0.98341214, -0.98397768, -0.9845335 , -0.98507957,\n       -0.98561591, -0.9861425 , -0.98665934, -0.98716641, -0.98766373,\n       -0.98815128, -0.98862906, -0.98909706, -0.98955528, -0.99000371,\n       -0.99044235, -0.9908712 , -0.99129025, -0.99169949, -0.99209893,\n       -0.99248855, -0.99286837, -0.99323836, -0.99359853, -0.99394887,\n       -0.99428939, -0.99462007, -0.99494091, -0.99525192, -0.99555308,\n       -0.9958444 , -0.99612587, -0.99639749, -0.99665925, -0.99691116,\n       -0.99715321, -0.9973854 , -0.99760773, -0.99782019, -0.99802278,\n       -0.9982155 , -0.99839835, -0.99857133, -0.99873443, -0.99888765,\n       -0.999031  , -0.99916446, -0.99928805, -0.99940175, -0.99950557,\n       -0.99959951, -0.99968356, -0.99975772, -0.999822  , -0.99987639,\n       -0.99992089, -0.9999555 , -0.99998022, -0.99999506, -1.        ])"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Self tuning penalty",
    "section": "",
    "text": "This is a research website. We are exploring the use of variational emprical Bayes to produce self tuning penalties, e.g. for multiple regression.\nInitial test"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "notebooks/vbmr_initial_test.html",
    "href": "notebooks/vbmr_initial_test.html",
    "title": "VBMR test",
    "section": "",
    "text": "import numpy as np\nimport jaxopt\nfrom vbmr.core import make_functions\nfrom vbmr.likelihoods import normal_log_likelihood, logistic_log_likliehood\nfrom vbmr.priors import normal_log_marginal_g\nimport matplotlib.pyplot as plt\nimport jax\nfrom functools import partial\nimport jax.numpy as jnp\n# give transformations for any tunable parameters\ndef softplus(x):\n    return jnp.log1p(jnp.exp(x))\n    \ntransforms = dict(\n    lparams=dict(scale = softplus),\n    gparams=dict(scale = softplus)\n)\n\nobjective, compute_posterior_moments, functions = make_functions(normal_log_likelihood, normal_log_marginal_g, transforms)\nget_range = lambda x: [x.min(), x.max()]\n\ndef simulate_gaussian(n, p, scale=1, scale0=1., seed=1):\n    np.random.seed(seed)\n    X = np.random.normal(size=(n, p))\n    b = np.random.normal(size=p) * scale0\n    y = X @ b + np.random.normal(size=n) * scale\n    return dict(y=y, X=X, b=b, scale=scale, scale0=scale0, seed=seed)\n    \n# the larger the prior precision, the more aggressive shrinkage.\nn, p = 1000, 10\nsim = simulate_gaussian(n, p, scale=1., scale0=1.)\ny, X, b = sim['y'], sim['X'], sim['b']\nWe still have a lot of work to do on setting up an optimizer for the objective that works reliabl. Here we use jaxopt.ScipyMinimize, but as we will see this can be unstable.\n#@partial(jax.jit, static_argnames=['objective', 'compute_posterior_moments'])\ndef scipy_fit(X, y, params, params_fixed, objective, compute_posterior_moments, maxiter=100):\n    p = X.shape[1]\n    solver = jaxopt.ScipyMinimize(fun=objective, maxiter=maxiter)\n    res = solver.run(params, X=X, y=y, params_fixed=params_fixed) # optimize  \n    mu, var = compute_posterior_moments(res.params, params_fixed)\n    return dict(res=res, mu=mu, var=var)"
  },
  {
    "objectID": "notebooks/vbmr_initial_test.html#gaussian-likelihood-gaussian-prior",
    "href": "notebooks/vbmr_initial_test.html#gaussian-likelihood-gaussian-prior",
    "title": "VBMR test",
    "section": "1 Gaussian likelihood, Gaussian prior",
    "text": "1 Gaussian likelihood, Gaussian prior\n\\[\n\\begin{aligned}\ny | b, {\\bf x}, \\sigma^2 \\sim N({\\bf x}^Tb, \\sigma^2) \\\\\n{\\bf b} \\sim N(0, \\sigma^2_0 I_p)\n\\end{aligned]\n\\]\nIn this case the CAVI updates are easy to compute. We can validate the results of our optimization aproach by comparing to CAVI approximate posterior. The posterior means agree, but the posterior variances are off by a factor of \\(2\\).\n\ndef kl_univariate_gaussian(muq, varq, varg):\n    return 0.5 * np.sum((varq + muq**2)/varg + np.log(varg/varq)  - 1)\n\ndef elbo_cavi(mu, var, y, X, tau, tau0):\n    ybar = X @ mu\n    d = np.diag(X.T @ X)\n    kl = kl_univariate_gaussian(mu, var, 1/tau0)\n    Eloglik =  -0.5 * tau * np.sum((y-ybar)**2) - 0.5 * tau * np.sum(d * var)\n    return Eloglik - kl\n\ndef gaussian_mr_cavi(X, y, scale, scale0, maxiter=100):\n    tau = 1/scale**2\n    tau0 = 1/scale0**2\n    n, p = X.shape\n    mu = np.zeros(X.shape[1])\n    var = np.ones(X.shape[1])\n    d = np.diag(X.T @ X)\n    r = y - X @ mu\n    elbos = [elbo_cavi(mu, var, y, X, tau, tau0)]\n    for i in range(maxiter):\n        for j in range(p):\n            r = r + X[:, j] * mu[j]\n            shrink = tau * d[j] /(tau * d[j] + tau0) \n            bhat = np.sum(r * X[:, j]) / d[j]\n            mu[j] = shrink * bhat\n            var[j] = 1/(tau * d[j] + tau0)\n            r = r - X[:,j] * mu[j]\n        elbos.append(elbo_cavi(mu, var, y, X, tau, tau0))\n    return dict(mu=mu, var=var, elbo=np.array(elbos), tau=tau, tau0=tau0)\n\n\nfig, ax = plt.subplots(1, 2)\n\nscale, scale0 = 1., 1.\ncavi_fit = gaussian_mr_cavi(X, y, scale, scale0, maxiter=100)\n\nparams = dict(\n    z=np.zeros(p), \n    s=np.ones(p)\n)\nparams_fixed = dict(\n    lparams=dict(scale=scale),\n    gparams=dict(scale=scale0)\n)\nfit1 = scipy_fit(X, y, params, params_fixed, objective, compute_posterior_moments, maxiter=10000)\n\nplt.sca(ax[0])\nplt.scatter(cavi_fit['mu'], fit1['mu'])\nplt.plot(get_range(cavi_fit['mu']), get_range(cavi_fit['mu']), color='k', linestyle='dotted')\nplt.sca(ax[1])\nplt.scatter(cavi_fit['var'], fit1['var'])\nplt.plot(get_range(cavi_fit['var']), get_range(cavi_fit['var']), color='k', linestyle='dotted')\n\n\n\n\n\n\n\n\n\ncavi_fit['var'] / fit1['var']\n\nArray([2.0014555, 1.9945623, 1.9970413, 2.0026052, 2.0013127, 2.0000277,\n       1.9962125, 1.9993687, 1.9994917, 1.9996684], dtype=float32)\n\n\n\n1.1 Estimating hyperparameters\nHere we try optimizing the prior variance of the effect. There is likely a bug, the optimization drives the prior variance to \\(0\\), so that all the effects are \\(0\\). Indeed the ELBO is better for the null solution than when the prior variance is fixed to it’s true value. There is probably a bug.\n\n# the larger the prior precision, the more aggressive shrinkage.\nn, p = 1000, 100\nsim = simulate_gaussian(n, p, scale=1., scale0=0.1)\ny, X, b = sim['y'], sim['X'], sim['b']\n\n# fixed prior variance\nparams = dict(\n    z=np.zeros(p), \n    s=np.ones(p)\n)\n\nscale0 = 0.1\nparams_fixed = dict(\n    lparams=dict(scale=1.0),\n    gparams=dict(scale=scale0)\n)\nfit1 = scipy_fit(X, y, params, params_fixed, objective, compute_posterior_moments)\n\n# estimate prior variance\nparams = dict(\n    z=np.zeros(p), \n    s=np.ones(p),\n    gparams=dict(scale=1.0)\n)\nparams_fixed = dict(\n    lparams=dict(scale=1.0),\n    gparams = dict()\n)\nfit2 = scipy_fit(X, y, params, params_fixed, objective, compute_posterior_moments)\n\nfig, ax = plt.subplots(1, 2)\n\nplt.sca(ax[0]); plt.scatter(b, fit1['mu'])\nplt.sca(ax[1]); plt.scatter(b, fit2['mu'])\n\n\n\n\n\n\n\n\n\nprint(f\"estimate prior variance: {softplus(fit2['res'].params['gparams']['scale'])**2:.2f}, -ELBO: {fit2['res'].state.fun_val:.2f}\")\nprint(f\"fixed prior variance: {scale0**2:.2f}, -ELBO: {fit1['res'].state.fun_val:.2f}\")\n\nestimate prior variance: 0.00, -ELBO: 1785.60\nfixed prior variance: 0.01, -ELBO: 2143.07"
  },
  {
    "objectID": "notebooks/vbmr_initial_test.html#logistic-regression",
    "href": "notebooks/vbmr_initial_test.html#logistic-regression",
    "title": "VBMR test",
    "section": "2 Logistic regression",
    "text": "2 Logistic regression\nHere we demonstrate fitting logistic regression with a gaussian prior. When the effects are small, we are able to optimize. When the effects are large the optimization fails and we predict huge posterior means. Perhaps notably, the estimated effect sizes correlate well with the true effect sizes.\n\ndef simulate_logistic(n, p, tau0=1., seed=1):\n    np.random.seed(seed)\n    X = np.random.normal(size=(n, p))\n    b = np.random.normal(size=p) / np.sqrt(tau0)\n    logit = X @ b\n    p = 1 / (1 + np.exp(-logit))\n    y = np.random.binomial(1, p)\n    return dict(y=y, X=X, b=b, tau0=tau0, seed=seed)\n    \n    transforms = dict(\n    tau0 = lambda x: jnp.log(1 + jnp.exp(x))\n)\n\n\nn, p = 1000, 100\nsim = simulate_logistic(n, p, tau0=100., seed=11)\ny, X, b = sim['y'], sim['X'], sim['b']\n\nlogistic_params_init = dict(\n    z = np.zeros(p),\n    s = np.ones(p),\n    gparams = dict(),\n    lparams = dict(),\n)\n\nlogistic_params_fixed = dict(\n    lparams = dict(), # residual error precision\n    gparams = dict(scale = 0.1) # prior effect precision\n)\n\nlogistic_objective, logistic_compute_posterior_moments, functions = make_functions(logistic_log_likliehood, normal_log_marginal_g, transforms)\nlogistic_fit = scipy_fit(X, y, logistic_params_init, logistic_params_fixed, logistic_objective, logistic_compute_posterior_moments, maxiter=10000)\nplt.scatter(b, logistic_fit['mu'])\nplt.plot(get_range(b), get_range(b), color='k', linestyle='dotted')\n\n\n\n\n\n\n\n\n\nsim = simulate_logistic(n, p, tau0=10., seed=11)\ny, X, b = sim['y'], sim['X'], sim['b']\n\nlogistic_params_init = dict(\n    z = np.zeros(p),\n    s = np.ones(p),\n    gparams = dict(),\n    lparams = dict(),\n)\n\nlogistic_params_fixed = dict(\n    lparams = dict(), # residual error precision\n    gparams = dict(scale = 0.1) # prior effect precision\n)\n\nlogistic_objective, logistic_compute_posterior_moments, functions = make_functions(logistic_log_likliehood, normal_log_marginal_g, transforms)\nlogistic_fit = scipy_fit(X, y, logistic_params_init, logistic_params_fixed, logistic_objective, logistic_compute_posterior_moments, maxiter=10000)\nplt.scatter(b, logistic_fit['mu'])\nplt.plot(get_range(b), get_range(b), color='k', linestyle='dotted')\n\n/Users/ktayeb/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:5540: RuntimeWarning: overflow encountered in cast\n  out = np.asarray(object, dtype=dtype)\n/Users/ktayeb/research/self-tuning-penalty/.venv/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:5540: RuntimeWarning: overflow encountered in cast\n  out = np.asarray(object, dtype=dtype)"
  }
]